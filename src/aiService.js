// AIæ¨¡å‹æœåŠ¡å…¼å®¹å±‚ - å¢å¼ºç‰ˆæœ¬
import { toolRegistry } from './utils/toolRegistry.js'

export class AIService {
    constructor(storageManager) {
        this.storageManager = storageManager
        this.requestQueue = []
        this.activeRequests = new Map()
        this.maxConcurrentRequests = 3 // æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
        this.toolRegistry = toolRegistry

        // æ”¯æŒçš„APIæä¾›å•†é…ç½®
        this.apiProviders = {
            openai: {
                name: 'OpenAI',
                baseUrl: 'https://api.openai.com/v1',
                chatEndpoint: '/chat/completions',
                models: [
                    'gpt-4', 'gpt-4-32k', 'gpt-4-turbo', 'gpt-4-vision-preview',
                    'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-instruct'
                ],
                recommendedModels: ['gpt-3.5-turbo', 'gpt-4o', 'gpt-4o-mini'],
                authHeader: 'Bearer',
                defaultModel: 'gpt-3.5-turbo',
                supportsCustomModel: true
            },
            deepseek: {
                name: 'DeepSeek',
                baseUrl: 'https://api.deepseek.com/v1',
                chatEndpoint: '/chat/completions',
                models: ['deepseek-chat', 'deepseek-reasoner'],
                recommendedModels: ['deepseek-chat'],
                authHeader: 'Bearer',
                defaultModel: 'deepseek-chat',
                supportsCustomModel: true
            },
            anthropic: {
                name: 'Anthropic',
                baseUrl: 'https://api.anthropic.com/v1',
                chatEndpoint: '/messages',
                models: [
                    'claude-3-haiku-20240307',
                    'claude-3-5-sonnet-20241022',
                    'claude-3-5-haiku-20241022',
                    'claude-3-opus-20240229',
                    'claude-3-sonnet-20240229'
                ],
                recommendedModels: ['claude-3-5-sonnet-20241022', 'claude-3-5-haiku-20241022'],
                authHeader: 'x-api-key',
                defaultModel: 'claude-3-5-sonnet-20241022',
                supportsCustomModel: true
            },
            azure: {
                name: 'Azure OpenAI',
                baseUrl: 'https://{resource}.openai.azure.com/openai/deployments/{deployment}',
                chatEndpoint: '/chat/completions',
                models: ['gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-35-turbo'],
                recommendedModels: ['gpt-4o', 'gpt-4o-mini'],
                authHeader: 'api-key',
                defaultModel: 'gpt-4o',
                supportsCustomModel: true
            },
            google: {
                name: 'Google Gemini',
                baseUrl: 'https://generativelanguage.googleapis.com/v1beta',
                chatEndpoint: '/models/{model}:generateContent',
                models: [
                    'gemini-1.5-pro',
                    'gemini-1.5-flash',
                    'gemini-pro',
                    'gemini-pro-vision'
                ],
                recommendedModels: ['gemini-1.5-pro', 'gemini-1.5-flash'],
                authHeader: 'x-goog-api-key',
                defaultModel: 'gemini-1.5-pro',
                supportsCustomModel: true
            },
            siliconflow: {
                name: 'ç¡…åŸºæµåŠ¨',
                baseUrl: 'https://api.siliconflow.cn/v1',
                chatEndpoint: '/chat/completions',
                models: [
                    // Qwen ç³»åˆ—
                    'Qwen/Qwen2.5-7B-Instruct',
                    'Qwen/Qwen2.5-14B-Instruct',
                    'Qwen/Qwen2.5-32B-Instruct',
                    'Qwen/Qwen2.5-72B-Instruct',
                    'Qwen/Qwen2.5-72B-Instruct-128K',
                    'Qwen/Qwen2-7B-Instruct',
                    'Qwen/Qwen3-8B',
                    'Qwen/Qwen3-14B',
                    'Qwen/Qwen3-30B-A3B',
                    'Qwen/Qwen3-32B',
                    'Qwen/Qwen3-235B-A22B',
                    'Qwen/Qwen3-30B-A3B-Instruct-2507',
                    'Qwen/Qwen3-235B-A22B-Instruct-2507',
                    'Qwen/Qwen3-235B-A22B-Thinking-2507',
                    'Qwen/Qwen3-30B-A3B-Thinking-2507',
                    'Qwen/Qwen3-Next-80B-A3B-Instruct',
                    'Qwen/Qwen3-Next-80B-A3B-Thinking',
                    'Qwen/Qwen3-Coder-30B-A3B-Instruct',
                    'Qwen/Qwen3-Coder-480B-A35B-Instruct',
                    'Qwen/Qwen2.5-Coder-7B-Instruct',
                    'Qwen/Qwen2.5-Coder-32B-Instruct',
                    'Qwen/QwQ-32B',
                    'Qwen/QVQ-72B-Preview',
                    'Tongyi-Zhiwen/QwenLong-L1-32B',
                    
                    // DeepSeek ç³»åˆ—
                    'deepseek-ai/DeepSeek-V3',
                    'deepseek-ai/DeepSeek-V2.5',
                    'deepseek-ai/DeepSeek-V3.1-Terminus',
                    'deepseek-ai/DeepSeek-V3.2',
                    'deepseek-ai/DeepSeek-R1',
                    'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',
                    'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B',
                    'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',
                    'deepseek-ai/DeepSeek-R1-0528-Qwen3-8B',
                    
                    // GLM ç³»åˆ—
                    'THUDM/glm-4-9b-chat',
                    'THUDM/GLM-4-9b-0414',
                    'THUDM/GLM-4-32B-0414',
                    'THUDM/GLM-4.1V-9B-Thinking',
                    'THUDM/GLM-Z1-9B-0414',
                    'THUDM/GLM-Z1-32B-0414',
                    'THUDM/GLM-Z1-Rumination-32B-0414',
                    'zai-org/GLM-4.5',
                    'zai-org/GLM-4.5-Air',
                    'zai-org/GLM-4.5V',
                    'zai-org/GLM-4.6',
                    'zai-org/GLM-4.6V',
                    
                    // ERNIE ç³»åˆ—
                    'baidu/ERNIE-4.5-21B-A3B-Paddle',
                    'baidu/ERNIE-4.5-300B-A47B',
                    'baidu/ERNIE-4.5-VL-28B-A3B-Paddle',
                    'baidu/ERNIE-4.5-VL-424B-A47B-Paddle',
                    
                    // Kimi ç³»åˆ—
                    'moonshotai/Kimi-Dev-72B',
                    'moonshotai/Kimi-K2-Instruct-0905',
                    'moonshotai/Kimi-K2-Thinking',
                    
                    // MiniMax ç³»åˆ—
                    'MiniMaxAI/MiniMax-M1-80k',
                    'MiniMaxAI/MiniMax-M2',
                    
                    // å…¶ä»– LLM æ¨¡å‹
                    'ascend-tribe/pangu-pro-moe',
                    'stepfun-ai/step3',
                    'internlm/internlm2_5-7b-chat',
                    'ByteDance-Seed/Seed-OSS-36B-Instruct',
                    'SeedLLM/Seed-Rice-7B',
                    'tencent/Hunyuan-A13B-Instruct',
                    'tencent/Hunyuan-MT-7B',
                    'inclusionAI/Ling-mini-2.0',
                    'inclusionAI/Ling-flash-2.0',
                    'inclusionAI/Ring-flash-2.0',
                    'Kwaipilot/KAT-Dev'
                ],
                recommendedModels: [
                    'Qwen/Qwen2.5-7B-Instruct', // æ¨èï¼šè½»é‡çº§ï¼Œé€Ÿåº¦å¿«
                    'Qwen/Qwen2.5-72B-Instruct', // æ¨èï¼šæ€§èƒ½å‡è¡¡
                    'deepseek-ai/DeepSeek-V3', // æ¨èï¼šæœ€æ–°ç‰ˆæœ¬ï¼Œæ€§èƒ½ä¼˜ç§€
                    'deepseek-ai/DeepSeek-R1', // æ¨èï¼šæ¨ç†èƒ½åŠ›å¼º
                    'THUDM/glm-4-9b-chat', // æ¨èï¼šè½»é‡çº§ï¼Œä¸­æ–‡ä¼˜ç§€
                    'moonshotai/Kimi-Dev-72B' // æ¨èï¼šé•¿æ–‡æœ¬èƒ½åŠ›å¼º
                ],
                authHeader: 'Bearer',
                defaultModel: 'Qwen/Qwen2.5-7B-Instruct',
                supportsCustomModel: true
            },
            local: {
                name: 'æœ¬åœ°éƒ¨ç½²',
                baseUrl: 'http://localhost:8080/v1',
                chatEndpoint: '/chat/completions',
                models: ['local-model'],
                authHeader: 'Bearer',
                defaultModel: 'local-model',
                supportsCustomModel: true
            }
        }
    }

    // å‘é€æ¶ˆæ¯åˆ°AIæ¨¡å‹ - æ”¯æŒå¹¶å‘è¯·æ±‚
    async sendMessage(agent, message, conversationHistory = [], onProgress = null) {
        const requestId = this.generateRequestId()

        return new Promise((resolve, reject) => {
            const request = {
                id: requestId,
                agent,
                message,
                conversationHistory,
                onProgress,
                resolve,
                reject,
                timestamp: Date.now()
            }

            this.requestQueue.push(request)
            this.processQueue()
        })
    }

    // ç”Ÿæˆå”¯ä¸€è¯·æ±‚ID
    generateRequestId() {
        return `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
    }

    // å¤„ç†è¯·æ±‚é˜Ÿåˆ—
    async processQueue() {
        // å¦‚æœå·²è¾¾åˆ°æœ€å¤§å¹¶å‘æ•°æˆ–é˜Ÿåˆ—ä¸ºç©ºï¼Œåˆ™è¿”å›
        if (this.activeRequests.size >= this.maxConcurrentRequests || this.requestQueue.length === 0) {
            return
        }

        // ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸‹ä¸€ä¸ªè¯·æ±‚
        const request = this.requestQueue.shift()
        this.activeRequests.set(request.id, request)

        try {
            const settings = this.storageManager.getSettings()
            const startTime = Date.now()
            let thinkingTime = 0

            console.log(`[AI Service] å¼€å§‹å¤„ç†è¯·æ±‚ - æ™ºèƒ½ä½“: ${request.agent.name}, ID: ${request.agent.id}`)

            // è·å–æ™ºèƒ½ä½“è®°å¿†å¹¶æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            let enhancedConversationHistory = [...request.conversationHistory]
            
            // è·å–æ™ºèƒ½ä½“è®°å¿†
            const agentMemory = this.storageManager.getAgentMemory(request.agent.id)
            console.log(`[AI Service] è·å–æ™ºèƒ½ä½“è®°å¿†:`, {
                agentId: request.agent.id,
                hasMemory: !!agentMemory,
                memoryLength: agentMemory?.content?.length || 0,
                conversationHistoryLength: enhancedConversationHistory.length
            })

            // æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦å·²ç»åŒ…å«è®°å¿†ç³»ç»Ÿæ¶ˆæ¯
            const hasMemoryMessage = enhancedConversationHistory.some(
                msg => msg.role === 'system' && msg.content && msg.content.includes('æ™ºèƒ½ä½“è®°å¿†')
            )
            console.log(`[AI Service] å¯¹è¯å†å²ä¸­æ˜¯å¦å·²åŒ…å«è®°å¿†æ¶ˆæ¯:`, hasMemoryMessage)

            // å¦‚æœæ™ºèƒ½ä½“æœ‰è®°å¿†ä¸”å¯¹è¯å†å²ä¸­è¿˜æ²¡æœ‰è®°å¿†æ¶ˆæ¯ï¼Œåˆ™æ³¨å…¥è®°å¿†
            if (agentMemory && agentMemory.content && agentMemory.content.trim() && !hasMemoryMessage) {
                const memorySystemMessage = {
                    role: 'system',
                    content: `æ™ºèƒ½ä½“è®°å¿†ï¼ˆç”¨äºæä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰:\n${agentMemory.content}\n\nè¯·åŸºäºä»¥ä¸Šè®°å¿†å†…å®¹ä¸ç”¨æˆ·è¿›è¡Œè‡ªç„¶çš„å¯¹è¯ï¼Œä¸è¦æ˜ç¡®æåŠè¿™äº›è®°å¿†ä¿¡æ¯ã€‚`
                }
                enhancedConversationHistory.unshift(memorySystemMessage)
                console.log(`[AI Service] å·²æ³¨å…¥æ™ºèƒ½ä½“è®°å¿†åˆ°å¯¹è¯å†å²å¼€å¤´`, {
                    memoryContentLength: agentMemory.content.length,
                    newHistoryLength: enhancedConversationHistory.length
                })
            } else if (hasMemoryMessage) {
                console.log(`[AI Service] å¯¹è¯å†å²ä¸­å·²å­˜åœ¨è®°å¿†æ¶ˆæ¯ï¼Œè·³è¿‡æ³¨å…¥`)
            } else if (!agentMemory || !agentMemory.content) {
                console.log(`[AI Service] æ™ºèƒ½ä½“æ— è®°å¿†å†…å®¹ï¼Œè·³è¿‡æ³¨å…¥`)
            }

            // åˆå¹¶è®¾ç½®ï¼šæ™ºèƒ½ä½“çº§åˆ«çš„è®¾ç½®ä¼˜å…ˆäºå…¨å±€è®¾ç½®
            const mergedSettings = this.mergeAgentSettings(settings, request.agent)
            console.log(`[AI Service] ä½¿ç”¨è®¾ç½®:`, {
                hasCustomApi: !!request.agent.useCustomApi,
                apiType: mergedSettings.apiType,
                apiEndpoint: mergedSettings.apiEndpoint ? 'å·²é…ç½®' : 'æœªé…ç½®',
                modelName: mergedSettings.modelName
            })

            let response
            if (mergedSettings.apiType === 'network') {
                if (mergedSettings.wordByWordOutput && request.onProgress) {
                    // æµå¼è¾“å‡ºæ¨¡å¼
                    response = await this.sendToNetworkAPIStream(request.agent, request.message, enhancedConversationHistory, mergedSettings, request.onProgress)
                    thinkingTime = Date.now() - startTime
                    response = this.addResponseMetadata(response, mergedSettings, thinkingTime)
                } else {
                    // æ™®é€šæ¨¡å¼
                    response = await this.sendToNetworkAPI(request.agent, request.message, enhancedConversationHistory, mergedSettings)
                    thinkingTime = Date.now() - startTime
                    response = this.addResponseMetadata(response, mergedSettings, thinkingTime)
                }
            } else {
                if (mergedSettings.wordByWordOutput && request.onProgress) {
                    // æœ¬åœ°æ¨¡å‹çš„æµå¼è¾“å‡º
                    response = await this.sendToLocalModelStream(request.agent, request.message, enhancedConversationHistory, mergedSettings, request.onProgress)
                    thinkingTime = Date.now() - startTime
                    response = this.addResponseMetadata(response, mergedSettings, thinkingTime)
                } else {
                    // æ™®é€šæ¨¡å¼
                    response = await this.sendToLocalModel(request.agent, request.message, enhancedConversationHistory, mergedSettings)
                    thinkingTime = Date.now() - startTime
                    response = this.addResponseMetadata(response, settings, thinkingTime)
                }
            }

            request.resolve(response)
        } catch (error) {
            request.reject(error)
        } finally {
            this.activeRequests.delete(request.id)
            // ç»§ç»­å¤„ç†é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯·æ±‚
            this.processQueue()
        }
    }

    // å–æ¶ˆæŒ‡å®šè¯·æ±‚
    cancelRequest(requestId) {
        // ä»é˜Ÿåˆ—ä¸­ç§»é™¤
        const queueIndex = this.requestQueue.findIndex(req => req.id === requestId)
        if (queueIndex !== -1) {
            this.requestQueue.splice(queueIndex, 1)
        }

        // ä»æ´»è·ƒè¯·æ±‚ä¸­ç§»é™¤
        if (this.activeRequests.has(requestId)) {
            this.activeRequests.delete(requestId)
        }
    }

    // è·å–å½“å‰è¯·æ±‚çŠ¶æ€
    getRequestStatus() {
        return {
            queueLength: this.requestQueue.length,
            activeRequests: this.activeRequests.size,
            maxConcurrent: this.maxConcurrentRequests
        }
    }

    // ç½‘ç»œAPIæµå¼è°ƒç”¨
    async sendToNetworkAPIStream(agent, message, conversationHistory, settings, onProgress) {
        const { apiEndpoint, apiKey, modelName, temperature, maxTokens } = settings

        // éªŒè¯åŸºæœ¬é…ç½®
        if (!apiEndpoint) {
            throw new Error('è¯·é…ç½®APIç«¯ç‚¹')
        }

        if (!apiKey) {
            throw new Error('è¯·é…ç½®APIå¯†é’¥')
        }

        // æ£€æµ‹APIæä¾›å•†
        const provider = this.detectAPIProvider(apiEndpoint)
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)

        // æ£€æŸ¥æ˜¯å¦éœ€è¦å¯ç”¨å·¥å…·è°ƒç”¨
        const enableTools = agent.skills && agent.skills.includes('webSearch')
        console.log(`[AI Service] æµå¼è¯·æ±‚ - å·¥å…·è°ƒç”¨æ£€æŸ¥:`, {
            agentName: agent.name,
            hasSkills: !!agent.skills,
            skills: agent.skills,
            enableTools
        })

        // æ„å»ºè¯·æ±‚ä½“ï¼Œå¯ç”¨æµå¼è¾“å‡º
        const requestBody = this.buildRequestBody(agent, message, conversationHistory, settings, provider, enableTools)
        requestBody.stream = true

        console.log(`[AI Service] å‘é€æµå¼ç½‘ç»œAPIè¯·æ±‚:`, {
            provider,
            url: fullUrl,
            model: modelName,
            messageLength: message.length,
            conversationHistoryLength: conversationHistory.length,
            requestBodyMessages: requestBody.messages ? requestBody.messages.length : 'N/A',
            enableTools,
            hasTools: !!requestBody.tools,
            toolsCount: requestBody.tools ? requestBody.tools.length : 0
        });

        // æ„å»ºè¯·æ±‚å¤´
        const headers = this.buildRequestHeaders(apiKey, provider)

        console.log(`ğŸ” å‘é€æµå¼è¯·æ±‚åˆ°: ${fullUrl}`)

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            if (!response.ok) {
                const errorInfo = await this.parseErrorResponse(response, provider)
                throw new Error(errorInfo)
            }

            // å¤„ç†æµå¼å“åº” - ä½¿ç”¨åŠ¨æ€é•¿åº¦æ§åˆ¶
            const reader = response.body.getReader()
            const decoder = new TextDecoder()
            let fullResponse = ''
            let fullReasoning = ''
            let buffer = ''
            let chunkCount = 0
            // æ ¹æ® maxTokens è®¡ç®—æœ€å¤§å“åº”é•¿åº¦ï¼ˆä¼°ç®—ï¼š1 token â‰ˆ 4 å­—ç¬¦ï¼‰
            const MAX_RESPONSE_LENGTH = (maxTokens || 2000) * 4
            const MAX_CHUNKS = 10000 // å¤§å¹…å¢åŠ  chunk æ•°é‡é™åˆ¶

            console.log(`[AI Service] æµå¼è¾“å‡ºé•¿åº¦æ§åˆ¶:`, {
                maxTokens,
                MAX_RESPONSE_LENGTH,
                MAX_CHUNKS
            })

            let lastUpdateTime = 0
            const UPDATE_INTERVAL = 50 // æœ€å°æ›´æ–°é—´éš”(ms)

            // ç”¨äºæ”¶é›†ä»¤ç‰Œæ•°ä¿¡æ¯
            let totalTokens = null

            // ç”¨äºæ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯
            let toolCallsBuffer = null
            let hasToolCalls = false

            while (true) {
                const { done, value } = await reader.read()
                if (done || chunkCount >= MAX_CHUNKS) break

                chunkCount++
                buffer += decoder.decode(value, { stream: true })
                const lines = buffer.split('\n')

                // ä¿ç•™æœ€åä¸€è¡Œï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰
                buffer = lines.pop() || ''

                for (const line of lines) {
                    if (line.startsWith('data: ') && line !== 'data: [DONE]') {
                        try {
                            const data = JSON.parse(line.substring(6))

                            // æå–ä»¤ç‰Œæ•°ä¿¡æ¯ï¼ˆç¡…åŸºæµåŠ¨åœ¨æ¯ä¸ªchunkä¸­éƒ½æä¾›ï¼‰
                            if (data.usage && data.usage.total_tokens) {
                                totalTokens = data.usage.total_tokens
                            }

                            // æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                            if (data.choices && data.choices[0] && data.choices[0].delta) {
                                const delta = data.choices[0].delta

                                // æ£€æŸ¥tool_calls
                                if (delta.tool_calls) {
                                    console.log(`[AI Service] æµå¼å“åº”ä¸­æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨:`, delta.tool_calls)
                                    hasToolCalls = true

                                    if (!toolCallsBuffer) {
                                        toolCallsBuffer = []
                                    }

                                    // å¤„ç†å·¥å…·è°ƒç”¨æ•°æ®
                                    delta.tool_calls.forEach((tc, index) => {
                                        if (!toolCallsBuffer[index]) {
                                            toolCallsBuffer[index] = {
                                                id: tc.id,
                                                type: tc.type,
                                                function: {
                                                    name: tc.function?.name || '',
                                                    arguments: tc.function?.arguments || ''
                                                }
                                            }
                                        } else {
                                            // è¿½åŠ å‚æ•°
                                            if (tc.function?.arguments) {
                                                toolCallsBuffer[index].function.arguments += tc.function.arguments
                                            }
                                        }
                                    })
                                }
                            }

                            const parsed = this.parseStreamResponseContent(data, provider)
                            if (parsed) {
                                // å¤„ç†å†…å®¹
                                if (parsed.content && fullResponse.length < MAX_RESPONSE_LENGTH) {
                                    fullResponse += parsed.content
                                }
                                // å¤„ç†æ€è€ƒå†…å®¹ï¼ˆé™åˆ¶ä¸ºæ€»é•¿åº¦çš„ 30%ï¼‰
                                const MAX_REASONING_LENGTH = Math.floor(MAX_RESPONSE_LENGTH * 0.3)
                                if (parsed.reasoning_content && fullReasoning.length < MAX_REASONING_LENGTH) {
                                    fullReasoning += parsed.reasoning_content
                                }

                                // ä½¿ç”¨èŠ‚æµæ§åˆ¶æ›´æ–°é¢‘ç‡
                                const now = Date.now()
                                if (now - lastUpdateTime >= UPDATE_INTERVAL) {
                                    // ä½¿ç”¨ç‰¹æ®Šæ ‡è®°åˆ†éš”æ€è€ƒå†…å®¹å’Œæ™®é€šå†…å®¹
                                    const combinedResponse = fullReasoning ? 
                                        `__REASONING_START__${fullReasoning}__REASONING_END__${fullResponse}` : 
                                        fullResponse
                                    onProgress(combinedResponse)
                                    lastUpdateTime = now
                                }
                            }
                        } catch (e) {
                            console.warn(`[AI Service] è§£ææµå¼æ•°æ®å¤±è´¥:`, e)
                        }
                    }
                }
            }

            // å¦‚æœæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œå¤„ç†å·¥å…·è°ƒç”¨
            if (hasToolCalls && toolCallsBuffer && toolCallsBuffer.length > 0) {
                console.log(`[AI Service] æµå¼å“åº”å®Œæˆï¼Œæ£€æµ‹åˆ° ${toolCallsBuffer.length} ä¸ªå·¥å…·è°ƒç”¨`)
                console.log(`[AI Service] å·¥å…·è°ƒç”¨è¯¦æƒ…:`, toolCallsBuffer)

                // æ„å»ºå®Œæ•´çš„å·¥å…·è°ƒç”¨å“åº”
                const toolCallsResponse = {
                    choices: [{
                        message: {
                            role: 'assistant',
                            content: fullResponse || null,
                            tool_calls: toolCallsBuffer
                        }
                    }]
                }

                return await this.handleToolCalls(agent, message, conversationHistory, settings, provider, toolCallsResponse)
            }

            // ç¡®ä¿æœ€ç»ˆæ–‡æœ¬å®Œæ•´æ˜¾ç¤º
            const combinedResponse = fullReasoning ? 
                `__REASONING_START__${fullReasoning}__REASONING_END__${fullResponse}` : 
                fullResponse
            onProgress(combinedResponse)

            console.log(`[AI Service] æµå¼è¾“å‡ºå®Œæˆ:`, {
                æ€»chunkæ•°: chunkCount,
                å“åº”é•¿åº¦: fullResponse.length,
                æ€è€ƒå†…å®¹é•¿åº¦: fullReasoning.length,
                æ€»é•¿åº¦: combinedResponse.length,
                æœ€å¤§é™åˆ¶: MAX_RESPONSE_LENGTH,
                ä»¤ç‰Œæ•°: totalTokens
            })

            // è¿”å›å“åº”å’Œä»¤ç‰Œæ•°
            return {
                response: combinedResponse,
                tokens: totalTokens
            }

        } catch (error) {
            console.error('ğŸ’¥ ç½‘ç»œAPIæµå¼è°ƒç”¨å¤±è´¥:', error)
            throw error
        }
    }

    // ç½‘ç»œAPIè°ƒç”¨ - å¢å¼ºç‰ˆæœ¬
    async sendToNetworkAPI(agent, message, conversationHistory, settings) {
        const { apiEndpoint, apiKey, modelName, temperature, maxTokens } = settings

        // éªŒè¯åŸºæœ¬é…ç½®
        if (!apiEndpoint) {
            throw new Error('è¯·é…ç½®APIç«¯ç‚¹')
        }

        if (!apiKey) {
            throw new Error('è¯·é…ç½®APIå¯†é’¥')
        }

        // éªŒè¯APIç«¯ç‚¹æ ¼å¼
        try {
            new URL(apiEndpoint)
        } catch {
            throw new Error('APIç«¯ç‚¹æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·è¾“å…¥æœ‰æ•ˆçš„URL')
        }

        // æ£€æµ‹APIæä¾›å•†
        const provider = this.detectAPIProvider(apiEndpoint)

        // æ„å»ºå®Œæ•´çš„è¯·æ±‚URL
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)

        // æ£€æŸ¥æ˜¯å¦éœ€è¦å¯ç”¨å·¥å…·è°ƒç”¨ï¼ˆæ™ºèƒ½ä½“æœ‰ç½‘ç»œæœç´¢æŠ€èƒ½ï¼‰
        const enableTools = agent.skills && agent.skills.includes('webSearch')

        // æ„å»ºè¯·æ±‚ä½“
        const requestBody = this.buildRequestBody(agent, message, conversationHistory, settings, provider, enableTools)

        console.log(`[AI Service] å‘é€ç½‘ç»œAPIè¯·æ±‚:`, {
            provider,
            url: fullUrl,
            model: modelName,
            messageLength: message.length,
            conversationHistoryLength: conversationHistory.length,
            requestBodyMessages: requestBody.messages ? requestBody.messages.length : 'N/A',
            enableTools,
            hasTools: !!requestBody.tools
        });

        // æ„å»ºè¯·æ±‚å¤´
        const headers = this.buildRequestHeaders(apiKey, provider)

        console.log(`ğŸ” å‘é€è¯·æ±‚åˆ°: ${fullUrl}`)

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            console.log(`ğŸ“¡ å“åº”çŠ¶æ€: ${response.status} ${response.statusText}`)

            if (!response.ok) {
                const errorInfo = await this.parseErrorResponse(response, provider)
                throw new Error(errorInfo)
            }

            const data = await response.json()
            console.log(`âœ… å“åº”æ•°æ®:`, data)

            // æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if (data.choices && data.choices[0] && data.choices[0].message && data.choices[0].message.tool_calls) {
                console.log(`[AI Service] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨è¯·æ±‚`)
                return await this.handleToolCalls(agent, message, conversationHistory, settings, provider, data)
            }

            // è§£æå“åº”å†…å®¹
            const parsed = this.parseResponseContent(data, provider)

            if (!parsed) {
                throw new Error('æ— æ³•è§£æAPIå“åº”å†…å®¹ï¼Œè¯·æ£€æŸ¥APIé…ç½®å’Œå“åº”æ ¼å¼')
            }

            // æå–ä»¤ç‰Œæ•°
            const tokens = data.usage ? data.usage.total_tokens : null

            // ä½¿ç”¨ç‰¹æ®Šæ ‡è®°åˆ†éš”æ€è€ƒå†…å®¹å’Œæ™®é€šå†…å®¹
            const combinedResponse = parsed.reasoning_content ? 
                `__REASONING_START__${parsed.reasoning_content}__REASONING_END__${parsed.content}` : 
                parsed.content

            // è¿”å›å“åº”å’Œä»¤ç‰Œæ•°
            return {
                response: combinedResponse,
                tokens: tokens
            }

        } catch (error) {
            console.error('ğŸ’¥ ç½‘ç»œAPIè°ƒç”¨å¤±è´¥:', error)

            // æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            if (error.name === 'TypeError' && error.message.includes('fetch')) {
                throw new Error('ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ï¼š\nâ€¢ APIç«¯ç‚¹æ˜¯å¦æ­£ç¡®\nâ€¢ ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\nâ€¢ æ˜¯å¦é‡åˆ°CORSé™åˆ¶')
            }

            if (error.message.includes('Failed to fetch')) {
                throw new Error('ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼Œå¯èƒ½åŸå› ï¼š\nâ€¢ APIç«¯ç‚¹æ— æ³•è®¿é—®\nâ€¢ ç½‘ç»œè¿æ¥é—®é¢˜\nâ€¢ æœåŠ¡å™¨æš‚æ—¶ä¸å¯ç”¨')
            }

            throw error
        }
    }

    // å¤„ç†å·¥å…·è°ƒç”¨
    async handleToolCalls(agent, message, conversationHistory, settings, provider, responseData) {
        const toolCalls = responseData.choices[0].message.tool_calls
        console.log(`[AI Service] ========== å¼€å§‹å¤„ç†å·¥å…·è°ƒç”¨ ==========`)
        console.log(`[AI Service] æ£€æµ‹åˆ° ${toolCalls.length} ä¸ªå·¥å…·è°ƒç”¨`)
        console.log(`[AI Service] å·¥å…·è°ƒç”¨è¯¦æƒ…:`, JSON.stringify(toolCalls, null, 2))

        // æ„å»ºæ–°çš„æ¶ˆæ¯å†å²ï¼ŒåŒ…å«å·¥å…·è°ƒç”¨è¯·æ±‚
        const newConversationHistory = [
            ...conversationHistory,
            { role: 'user', content: message },
            responseData.choices[0].message
        ]

        console.log(`[AI Service] æ–°çš„æ¶ˆæ¯å†å²é•¿åº¦: ${newConversationHistory.length}`)

        // æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        const toolResults = []
        for (let i = 0; i < toolCalls.length; i++) {
            const toolCall = toolCalls[i]
            const functionName = toolCall.function.name
            const functionArgs = JSON.parse(toolCall.function.arguments)

            console.log(`[AI Service] ---------- æ‰§è¡Œå·¥å…· ${i + 1}/${toolCalls.length} ----------`)
            console.log(`[AI Service] å·¥å…·åç§°: ${functionName}`)
            console.log(`[AI Service] å·¥å…·å‚æ•°:`, functionArgs)

            try {
                console.log(`[AI Service] è°ƒç”¨å·¥å…·æ³¨å†Œè¡¨æ‰§è¡Œå·¥å…·...`)
                const result = await this.toolRegistry.executeTool(functionName, functionArgs)

                console.log(`[AI Service] å·¥å…·æ‰§è¡Œç»“æœ:`, result)
                console.log(`[AI Service] å·¥å…·æ‰§è¡ŒæˆåŠŸ: ${functionName}`)

                toolResults.push({
                    tool_call_id: toolCall.id,
                    role: 'tool',
                    name: functionName,
                    content: JSON.stringify(result)
                })
            } catch (error) {
                console.error(`[AI Service] å·¥å…·æ‰§è¡Œå¤±è´¥: ${functionName}`, error)
                console.error(`[AI Service] é”™è¯¯è¯¦æƒ…:`, error.message, error.stack)

                toolResults.push({
                    tool_call_id: toolCall.id,
                    role: 'tool',
                    name: functionName,
                    content: JSON.stringify({ error: error.message })
                })
            }
        }

        console.log(`[AI Service] æ‰€æœ‰å·¥å…·æ‰§è¡Œå®Œæˆï¼Œå…± ${toolResults.length} ä¸ªç»“æœ`)
        console.log(`[AI Service] å·¥å…·ç»“æœ:`, toolResults)

        // å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
        const updatedConversationHistory = [
            ...newConversationHistory,
            ...toolResults
        ]

        console.log(`[AI Service] æ›´æ–°åçš„æ¶ˆæ¯å†å²é•¿åº¦: ${updatedConversationHistory.length}`)

        // å†æ¬¡è°ƒç”¨APIï¼Œè®©æ¨¡å‹åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤
        console.log(`[AI Service] ========== åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›å¤ ==========`)

        const { apiEndpoint, apiKey, modelName, temperature, maxTokens } = settings
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)

        console.log(`[AI Service] è¯·æ±‚URL: ${fullUrl}`)
        console.log(`[AI Service] è¯·æ±‚æ¨¡å‹: ${modelName}`)

        const requestBody = this.buildRequestBody(agent, '', updatedConversationHistory, settings, provider, false)
        const headers = this.buildRequestHeaders(apiKey, provider)

        console.log(`[AI Service] è¯·æ±‚ä½“:`, JSON.stringify(requestBody, null, 2))

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            console.log(`[AI Service] æœ€ç»ˆå“åº”çŠ¶æ€: ${response.status} ${response.statusText}`)

            if (!response.ok) {
                const errorInfo = await this.parseErrorResponse(response, provider)
                console.error(`[AI Service] æœ€ç»ˆè¯·æ±‚å¤±è´¥:`, errorInfo)
                throw new Error(errorInfo)
            }

            const data = await response.json()
            console.log(`[AI Service] æœ€ç»ˆå“åº”æ•°æ®:`, data)

            const content = this.parseResponseContent(data, provider)
            console.log(`[AI Service] è§£æåçš„å†…å®¹:`, content)
            console.log(`[AI Service] ========== å·¥å…·è°ƒç”¨æµç¨‹å®Œæˆ ==========`)

            return content
        } catch (error) {
            console.error(`[AI Service] æœ€ç»ˆè¯·æ±‚å¼‚å¸¸:`, error)
            throw error
        }
    }

    // æ£€æµ‹APIæä¾›å•†
    detectAPIProvider(apiEndpoint) {
        const endpoint = apiEndpoint.toLowerCase()

        if (endpoint.includes('openai.com')) {
            return 'openai'
        } else if (endpoint.includes('deepseek.com')) {
            return 'deepseek'
        } else if (endpoint.includes('anthropic.com')) {
            return 'anthropic'
        } else if (endpoint.includes('azure.com') || endpoint.includes('openai.azure.com')) {
            return 'azure'
        } else if (endpoint.includes('googleapis.com') || endpoint.includes('generativelanguage.googleapis.com')) {
            return 'google'
        } else if (endpoint.includes('siliconflow.cn')) {
            return 'siliconflow'
        } else if (endpoint.includes('localhost') || endpoint.includes('127.0.0.1')) {
            return 'local'
        } else {
            return 'custom'
        }
    }

    // æ„å»ºè¯·æ±‚URL
    buildRequestUrl(apiEndpoint, provider) {
        if (provider === 'custom') {
            return apiEndpoint
        }

        const providerConfig = this.apiProviders[provider]
        if (!providerConfig) {
            return apiEndpoint
        }

        // å¦‚æœç”¨æˆ·æä¾›äº†å®Œæ•´çš„ç«¯ç‚¹ï¼Œç›´æ¥ä½¿ç”¨
        if (apiEndpoint.includes(providerConfig.chatEndpoint)) {
            return apiEndpoint
        }

        // å¦åˆ™æ„å»ºå®Œæ•´URL
        return apiEndpoint.endsWith('/')
            ? `${apiEndpoint}${providerConfig.chatEndpoint.substring(1)}`
            : `${apiEndpoint}${providerConfig.chatEndpoint}`
    }

    // æ„å»ºè¯·æ±‚ä½“

        buildRequestBody(agent, message, conversationHistory, settings, provider, enableTools = false) {

            const { modelName, temperature, maxTokens } = settings

    

            const messages = this.buildMessages(agent, message, conversationHistory, settings)

    

            // ç¡®ä¿æ•°å€¼å‚æ•°ä¸ºæ•°å­—ç±»å‹

            const tempValue = Number(temperature) || 0.7

            

            // æ™ºèƒ½ maxTokens è°ƒæ•´æœºåˆ¶

            let maxTokensValue = Number(maxTokens) || 2000

            

            // è·å–æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°

            const modelContextWindow = this.getModelContextWindow(modelName)

            

            // è®¡ç®—å½“å‰è¯·æ±‚çš„å®é™… token æ¶ˆè€—

            let requestTokens = 0

            messages.forEach(msg => {

                requestTokens += this.estimateTokens(msg.content || '')

            })

            

            // è®¡ç®—å¯ç”¨äºè¾“å‡ºçš„ token é¢„ç®—ï¼ˆä¿ç•™ 30% ç¼“å†²ï¼‰

            const outputTokenBudget = Math.floor((modelContextWindow - requestTokens) * 0.7)

            

            // å¦‚æœç”¨æˆ·è®¾ç½®çš„ maxTokens è¶…å‡ºæ¨¡å‹çš„å®é™…èƒ½åŠ›ï¼Œè‡ªåŠ¨è°ƒæ•´

            if (maxTokensValue > outputTokenBudget) {

                console.log(`[AI Service] æ™ºèƒ½è°ƒæ•´ maxTokens:`, {

                    ç”¨æˆ·è®¾ç½®: maxTokensValue,

                    æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£: modelContextWindow,

                    è¯·æ±‚tokenæ•°: requestTokens,

                    è¾“å‡ºtokené¢„ç®—: outputTokenBudget,

                    è°ƒæ•´å: outputTokenBudget

                });

                maxTokensValue = outputTokenBudget

            }

            

            // ç¡®ä¿ maxTokens ä¸ä½äºæœ€å°å€¼ 100

            

                        maxTokensValue = Math.max(100, maxTokensValue)

            

            

            

                        // æ ¹æ®æœåŠ¡å•†é™åˆ¶ maxTokens çš„æœ€å¤§å€¼

            

                        const providerMaxTokensLimits = {

            

                            deepseek: 8192,

            

                            anthropic: 4096,

            

                            openai: 4096,

            

                            azure: 4096,

            

                            google: 8192

            

                        }

            

            

            

                        if (providerMaxTokensLimits[provider]) {

            

                            maxTokensValue = Math.min(maxTokensValue, providerMaxTokensLimits[provider])

            

                            console.log(`[AI Service] åº”ç”¨æœåŠ¡å•† maxTokens é™åˆ¶:`, {

            

                                æœåŠ¡å•†: provider,

            

                                é™åˆ¶: providerMaxTokensLimits[provider],

            

                                æœ€ç»ˆå€¼: maxTokensValue

            

                            })

            

                        }

    

            // åŸºç¡€è¯·æ±‚ä½“

            let requestBody = {

                model: modelName,

                messages: messages,

                temperature: tempValue,

                max_tokens: maxTokensValue,

                stream: false

            }

        // å¦‚æœå¯ç”¨äº†å·¥å…·ä¸”æ™ºèƒ½ä½“æœ‰ç½‘ç»œæœç´¢æŠ€èƒ½ï¼Œæ·»åŠ å·¥å…·å®šä¹‰
        if (enableTools && agent.skills && agent.skills.includes('webSearch')) {
            const tools = this.toolRegistry.getOpenAITools()
            if (tools.length > 0) {
                requestBody.tools = tools
                requestBody.tool_choice = 'auto' // è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
            }
        }

        // æä¾›å•†ç‰¹å®šé…ç½®
        switch (provider) {
            case 'anthropic':
                requestBody = {
                    model: modelName,
                    messages: messages,
                    max_tokens: maxTokensValue,
                    temperature: tempValue
                }
                // Anthropicä¹Ÿæ”¯æŒå·¥å…·è°ƒç”¨
                if (enableTools && agent.skills && agent.skills.includes('webSearch')) {
                    const tools = this.toolRegistry.getOpenAITools()
                    if (tools.length > 0) {
                        requestBody.tools = tools
                    }
                }
                break

            case 'google':
                requestBody = {
                    contents: messages.map(msg => ({
                        parts: [{ text: msg.content }],
                        role: msg.role === 'user' ? 'user' : 'model'
                    }))
                }
                // Geminiçš„å‡½æ•°è°ƒç”¨æ ¼å¼ä¸åŒ
                if (enableTools && agent.skills && agent.skills.includes('webSearch')) {
                    requestBody.tools = this.toolRegistry.getAllTools().map(tool => ({
                        functionDeclarations: [{
                            name: tool.name,
                            description: tool.description,
                            parameters: tool.parameters
                        }]
                    }))
                }
                break

            case 'azure':
                // Azureä½¿ç”¨api-versionå‚æ•°
                requestBody.api_version = '2023-12-01-preview'
                break
        }

        return requestBody
    }

    // æ„å»ºè¯·æ±‚å¤´
    buildRequestHeaders(apiKey, provider) {
        const headers = {
            'Content-Type': 'application/json'
        }

        const providerConfig = this.apiProviders[provider]
        if (providerConfig && apiKey) {
            headers[providerConfig.authHeader === 'x-api-key' ? 'x-api-key' : 'Authorization'] =
                providerConfig.authHeader === 'x-api-key' ? apiKey : `${providerConfig.authHeader} ${apiKey}`
        }

        // ç‰¹æ®Šå¤´éƒ¨
        switch (provider) {
            case 'anthropic':
                headers['anthropic-version'] = '2023-06-01'
                break
        }

        return headers
    }

    // è§£æé”™è¯¯å“åº”
    async parseErrorResponse(response, provider) {
        let errorMessage = `APIè¯·æ±‚å¤±è´¥: ${response.status} ${response.statusText}`

        try {
            const errorData = await response.json()
            console.log('ğŸ” é”™è¯¯å“åº”æ•°æ®:', errorData)

            if (errorData.error && errorData.error.message) {
                errorMessage = `${errorData.error.message}`
                        } else if (errorData.message) {
                            errorMessage = `${errorData.message}`
                        } else if (errorData.detail) {
                            errorMessage = `${errorData.detail}`            }

            // å¸¸è§é”™è¯¯ä»£ç å¤„ç†
            if (response.status === 401) {
                errorMessage += '\nè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®'
            } else if (response.status === 403) {
                errorMessage += '\næƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æƒé™'
            } else if (response.status === 404) {
                errorMessage += '\nèµ„æºæœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥APIç«¯ç‚¹æ˜¯å¦æ­£ç¡®'
            } else if (response.status === 429) {
                errorMessage += '\nè¯·æ±‚é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•'
            } else if (response.status >= 500) {
                errorMessage += '\næœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•'
            }

        } catch {
            // å¦‚æœæ— æ³•è§£æé”™è¯¯å“åº”ï¼Œä½¿ç”¨é»˜è®¤é”™è¯¯ä¿¡æ¯
            console.warn('âš ï¸ æ— æ³•è§£æé”™è¯¯å“åº”')
        }

        return errorMessage
    }

    // è§£ææµå¼å“åº”å†…å®¹
    parseStreamResponseContent(data, provider) {
        console.log('ğŸ” è§£ææµå¼æ•°æ®:', data)

        // å…¼å®¹ä¸åŒAPIæä¾›å•†çš„æµå¼å“åº”æ ¼å¼
        switch (provider) {
            case 'openai':
            case 'deepseek':
            case 'azure':
            case 'local':
            case 'siliconflow':
                if (data.choices && data.choices[0] && data.choices[0].delta) {
                    const delta = data.choices[0].delta
                    // è¿”å›åŒ…å«å†…å®¹å’Œæ€è€ƒå†…å®¹çš„å¯¹è±¡
                    return {
                        content: delta.content || '',
                        reasoning_content: delta.reasoning_content || ''
                    }
                }
                break

            case 'anthropic':
                if (data.type === 'content_block_delta' && data.delta && data.delta.text) {
                    return {
                        content: data.delta.text,
                        reasoning_content: ''
                    }
                }
                break

            case 'google':
                if (data.candidates && data.candidates[0] && data.candidates[0].content) {
                    return {
                        content: data.candidates[0].content.parts[0].text,
                        reasoning_content: ''
                    }
                }
                break

            case 'custom':
                // å°è¯•å¤šç§å¸¸è§æ ¼å¼
                if (data.choices?.[0]?.delta) {
                    const delta = data.choices[0].delta
                    return {
                        content: delta.content || '',
                        reasoning_content: delta.reasoning_content || ''
                    }
                } else if (data.delta?.content) {
                    return {
                        content: data.delta.content,
                        reasoning_content: data.delta.reasoning_content || ''
                    }
                } else if (data.content) {
                    return {
                        content: data.content,
                        reasoning_content: ''
                    }
                }
                break
        }

        return {
            content: '',
            reasoning_content: ''
        }
    }

    // è§£æå“åº”å†…å®¹

        parseResponseContent(data, provider) {

            console.log('ğŸ” è§£æå“åº”æ•°æ®:', data)

    

            // å…¼å®¹ä¸åŒAPIæä¾›å•†çš„å“åº”æ ¼å¼

            switch (provider) {

                case 'openai':

                case 'deepseek':

                case 'azure':

                case 'local':

                case 'siliconflow':

                    if (data.choices && data.choices[0] && data.choices[0].message) {

                        const message = data.choices[0].message

                        return {

                            content: message.content || '',

                            reasoning_content: message.reasoning_content || ''

                        }

                    }

                    break

    

                case 'anthropic':

                    if (data.content && data.content[0] && data.content[0].text) {

                        return {

                            content: data.content[0].text,

                            reasoning_content: ''

                        }

                    }

                    break

    

                case 'google':

                    if (data.candidates && data.candidates[0] && data.candidates[0].content) {

                        return {

                            content: data.candidates[0].content.parts[0].text,

                            reasoning_content: ''

                        }

                    }

                    break

    

                case 'custom':

                    // å°è¯•å¤šç§å¸¸è§æ ¼å¼

                    if (data.choices?.[0]?.message) {

                        const message = data.choices[0].message

                        return {

                            content: message.content || '',

                            reasoning_content: message.reasoning_content || ''

                        }

                    } else if (data.content) {

                        return {

                            content: data.content,

                            reasoning_content: ''

                        }

                    } else if (data.result) {

                        return {

                            content: data.result,

                            reasoning_content: ''

                        }

                    } else if (data.text) {

                        return {

                            content: data.text,

                            reasoning_content: ''

                        }

                    }

                    break

            }

    

            console.warn('âš ï¸ æœªçŸ¥çš„APIå“åº”æ ¼å¼:', data)

            return {

                content: '',

                reasoning_content: ''

            }

        }

    // è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
    getSupportedModels(apiEndpoint) {
        const provider = this.detectAPIProvider(apiEndpoint)
        const providerConfig = this.apiProviders[provider]

        if (providerConfig) {
            return providerConfig.models
        }

        // å¯¹äºè‡ªå®šä¹‰ç«¯ç‚¹ï¼Œè¿”å›é€šç”¨æ¨¡å‹åˆ—è¡¨
        return [
            'gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo',
            'deepseek-chat', 'deepseek-coder',
            'claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku',
            'gemini-pro', 'custom-model'
        ]
    }

    // è·å–APIæä¾›å•†ä¿¡æ¯
    getAPIProviderInfo(apiEndpoint) {
        const provider = this.detectAPIProvider(apiEndpoint)
        return this.apiProviders[provider] || {
            name: 'è‡ªå®šä¹‰API',
            models: ['custom-model']
        }
    }

    // æœ¬åœ°æ¨¡å‹æµå¼è°ƒç”¨ - ä¼˜åŒ–ç‰ˆæœ¬
    async sendToLocalModelStream(agent, message, conversationHistory, settings, onProgress) {
        const messages = this.buildMessages(agent, message, conversationHistory, settings)

        console.log(`[AI Service] å‘é€æµå¼æœ¬åœ°æ¨¡å‹è¯·æ±‚:`, {
            model: 'local-model',
            messageLength: message.length,
            conversationHistoryLength: conversationHistory.length,
            messagesUsed: messages.length
        });

        // åŸºäºæ™ºèƒ½ä½“é…ç½®ç”Ÿæˆå›å¤
        const context = agent.prompt || ''
        const scenario = agent.scenario || ''
        const keyPoints = agent.keyPoints || ''

        let fullResponse = ''

        if (context.includes('åŠ©æ‰‹') || context.includes('assistant')) {
            fullResponse = this.generateHelpfulResponse(message, context)
        } else if (context.includes('æœ‹å‹') || context.includes('friend')) {
            fullResponse = this.generateFriendlyResponse(message, context)
        } else if (context.includes('ä¸“å®¶') || context.includes('expert')) {
            fullResponse = this.generateExpertResponse(message, context)
        } else {
            fullResponse = this.generateDefaultResponse(message, context)
        }

        // æ·»åŠ åœºæ™¯å’Œè¦ç‚¹ä¿¡æ¯
        if (scenario) {
            fullResponse = `[åœºæ™¯: ${scenario}] ${fullResponse}`
        }

        if (keyPoints) {
            fullResponse += `\n\n[è¦ç‚¹æé†’: ${keyPoints}]`
        }

        // ä¼˜åŒ–çš„æµå¼è¾“å‡º - ä½¿ç”¨å­—ç¬¦çº§è¾“å‡ºï¼Œä½†å‡å°‘æ›´æ–°é¢‘ç‡
        let currentText = ''
        const chars = fullResponse.split('')
        let lastUpdateTime = 0
        const UPDATE_INTERVAL = 50 // æœ€å°æ›´æ–°é—´éš”(ms)

        for (let i = 0; i < chars.length; i++) {
            currentText += chars[i]

            // ä½¿ç”¨èŠ‚æµæ§åˆ¶æ›´æ–°é¢‘ç‡
            const now = Date.now()
            if (now - lastUpdateTime >= UPDATE_INTERVAL) {
                onProgress(currentText)
                lastUpdateTime = now
            }

            // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿï¼Œä½†ä½¿ç”¨æ›´åˆç†çš„å»¶è¿Ÿæ—¶é—´
            await new Promise(resolve => setTimeout(resolve, 30 + Math.random() * 30))
        }

        // ç¡®ä¿æœ€ç»ˆæ–‡æœ¬å®Œæ•´æ˜¾ç¤º
        if (currentText !== fullResponse) {
            onProgress(fullResponse)
        }

        return fullResponse
    }

    // æœ¬åœ°æ¨¡å‹è°ƒç”¨ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
    async sendToLocalModel(agent, message, conversationHistory, settings) {
        const messages = this.buildMessages(agent, message, conversationHistory, settings)
        
        console.log(`[AI Service] å‘é€æœ¬åœ°æ¨¡å‹è¯·æ±‚:`, {
            model: 'local-model',
            messageLength: message.length,
            conversationHistoryLength: conversationHistory.length,
            messagesUsed: messages.length
        });

        // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 1000))

        // åŸºäºæ™ºèƒ½ä½“é…ç½®ç”Ÿæˆå›å¤
        const context = agent.prompt || ''
        const scenario = agent.scenario || ''
        const keyPoints = agent.keyPoints || ''

        let response = ''

        if (context.includes('åŠ©æ‰‹') || context.includes('assistant')) {
            response = this.generateHelpfulResponse(message, context)
        } else if (context.includes('æœ‹å‹') || context.includes('friend')) {
            response = this.generateFriendlyResponse(message, context)
        } else if (context.includes('ä¸“å®¶') || context.includes('expert')) {
            response = this.generateExpertResponse(message, context)
        } else {
            response = this.generateDefaultResponse(message, context)
        }

        // æ·»åŠ åœºæ™¯å’Œè¦ç‚¹ä¿¡æ¯
        if (scenario) {
            response = `[åœºæ™¯: ${scenario}] ${response}`
        }

        if (keyPoints) {
            response += `\n\n[è¦ç‚¹æé†’: ${keyPoints}]`
        }

        return response
    }

    // æ„å»ºæ¶ˆæ¯æ•°ç»„
    buildMessages(agent, currentMessage, conversationHistory, settings = null) {
        // è·å–ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼Œå¦‚æœæ²¡æœ‰æä¾›settingsåˆ™ä½¿ç”¨é»˜è®¤å€¼50
        const maxHistoryLength = settings && settings.contextLength ? settings.contextLength : 50
        
        // è·å–æœ€å¤§è¾“å‡º token æ•°ï¼Œç”¨äºè®¡ç®—ä¸Šä¸‹æ–‡é¢„ç®—
        const maxOutputTokens = settings && settings.maxTokens ? settings.maxTokens : 2000
        
        // ä¼°ç®—æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
        // OpenAI GPT-4: 128K, GPT-3.5: 16K, DeepSeek: 128K, Claude: 200K
        const modelContextWindow = this.getModelContextWindow(settings?.modelName || 'gpt-3.5-turbo')
        
        // è®¡ç®—å¯ç”¨çš„ä¸Šä¸‹æ–‡ token é¢„ç®—ï¼ˆä¿ç•™ 20% ç¼“å†²ï¼‰
        const contextBudget = Math.floor(modelContextWindow * 0.8) - maxOutputTokens
        
        // ä¼°ç®—ç³»ç»Ÿæç¤ºè¯çš„ token æ•°
        const systemPromptTokens = agent.prompt ? this.estimateTokens(agent.prompt) : 0
        
        // ä¼°ç®—å½“å‰æ¶ˆæ¯çš„ token æ•°
        const currentMessageTokens = this.estimateTokens(currentMessage)
        
        // è®¡ç®—å¯ç”¨äºå†å²æ¶ˆæ¯çš„ token é¢„ç®—
        const historyTokenBudget = contextBudget - systemPromptTokens - currentMessageTokens
        
        // é™åˆ¶å¯¹è¯å†å²é•¿åº¦ï¼Œåªå–æœ€è¿‘çš„æ¶ˆæ¯
        let recentHistory = conversationHistory.slice(-maxHistoryLength)
        
        // å¦‚æœå†å²æ¶ˆæ¯çš„ token æ•°è¶…å‡ºé¢„ç®—ï¼Œè¿›è¡Œæ™ºèƒ½æˆªæ–­
        let historyTokens = 0
        const truncatedHistory = []
        
        // ä»æœ€æ–°çš„æ¶ˆæ¯å¼€å§‹å€’åºéå†ï¼Œç¡®ä¿ä¿ç•™æœ€æ–°çš„å¯¹è¯
        for (let i = recentHistory.length - 1; i >= 0; i--) {
            const msg = recentHistory[i]
            const msgTokens = this.estimateTokens(msg.content || '')
            
            // å¦‚æœæ·»åŠ è¿™æ¡æ¶ˆæ¯ä¼šè¶…å‡ºé¢„ç®—ï¼Œè·³è¿‡
            if (historyTokens + msgTokens > historyTokenBudget) {
                continue
            }
            
            truncatedHistory.unshift(msg)
            historyTokens += msgTokens
        }
        
        recentHistory = truncatedHistory

        // è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºæˆªæ–­å‰åçš„å†å²æ¶ˆæ¯æ•°é‡å’Œ token æ•°
        if (conversationHistory.length > recentHistory.length) {
            console.log(`[AI Service] æ¶ˆæ¯å†å²å·²æˆªæ–­:`, {
                åŸå§‹æ¶ˆæ¯æ•°: conversationHistory.length,
                æˆªæ–­åæ¶ˆæ¯æ•°: recentHistory.length,
                æ¶ˆæ¯æ•°é™åˆ¶: maxHistoryLength,
                æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£: modelContextWindow,
                ä¸Šä¸‹æ–‡é¢„ç®—: contextBudget,
                ç³»ç»Ÿæç¤ºè¯token: systemPromptTokens,
                å½“å‰æ¶ˆæ¯token: currentMessageTokens,
                å†å²æ¶ˆæ¯tokené¢„ç®—: historyTokenBudget,
                å®é™…å†å²æ¶ˆæ¯token: historyTokens
            });
        } else {
            console.log(`[AI Service] æ¶ˆæ¯å†å²æœªæˆªæ–­:`, {
                æ¶ˆæ¯æ•°: conversationHistory.length,
                æ¶ˆæ¯æ•°é™åˆ¶: maxHistoryLength,
                å®é™…å†å²æ¶ˆæ¯token: historyTokens,
                å†å²æ¶ˆæ¯tokené¢„ç®—: historyTokenBudget
            });
        }

        const messages = []

        // ç³»ç»Ÿæç¤ºè¯
        if (agent.prompt) {
            messages.push({
                role: 'system',
                content: agent.prompt
            })
        }

        // å¯¹è¯å†å²
        recentHistory.forEach(msg => {
            const messageObj = {
                role: msg.role,
                content: msg.content
            }

            // ä¿ç•™å·¥å…·æ¶ˆæ¯çš„tool_call_idå­—æ®µ
            if (msg.tool_call_id) {
                messageObj.tool_call_id = msg.tool_call_id
            }

            // ä¿ç•™å·¥å…·è°ƒç”¨çš„å…¶ä»–å­—æ®µ
            if (msg.tool_calls) {
                messageObj.tool_calls = msg.tool_calls
            }

            messages.push(messageObj)
        })

        // å½“å‰æ¶ˆæ¯
        messages.push({
            role: 'user',
            content: currentMessage
        })

        // è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºæ„å»ºçš„æ€»æ¶ˆæ¯æ•°é‡
        console.log(`[AI Service] æ„å»ºçš„æ¶ˆæ¯æ€»æ•°: ${messages.length} (ç³»ç»Ÿæ¶ˆæ¯: ${agent.prompt ? 1 : 0}, å†å²æ¶ˆæ¯: ${recentHistory.length}, å½“å‰æ¶ˆæ¯: 1)`);
        
        return messages
    }

    // æ¨¡æ‹Ÿå›å¤ç”Ÿæˆå‡½æ•°
    generateHelpfulResponse(message, context) {
        const responses = [
            `æˆ‘ç†è§£æ‚¨çš„éœ€æ±‚ã€‚${message} è¿™ä¸ªé—®é¢˜å¯ä»¥ä»å¤šä¸ªè§’åº¦æ¥åˆ†æã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘...`,
            `æ„Ÿè°¢æ‚¨çš„æé—®ã€‚å…³äº"${message}"ï¼Œæˆ‘çš„å»ºè®®æ˜¯...`,
            `è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚è®©æˆ‘ä¸ºæ‚¨è¯¦ç»†è§£é‡Šä¸€ä¸‹${message}çš„ç›¸å…³å†…å®¹...`,
            `æˆ‘æ³¨æ„åˆ°æ‚¨æåˆ°${message}ã€‚æ ¹æ®æˆ‘çš„çŸ¥è¯†ï¼Œè¿™é‡Œæœ‰å‡ ä¸ªè¦ç‚¹éœ€è¦å…³æ³¨...`
        ]
        return responses[Math.floor(Math.random() * responses.length)]
    }

    generateFriendlyResponse(message, context) {
        const responses = [
            `å“ˆå“ˆï¼Œ${message} è¿™ä¸ªè¯é¢˜å¾ˆæœ‰æ„æ€ï¼æˆ‘è§‰å¾—...`,
            `æœ‹å‹ï¼Œå…³äº${message}ï¼Œæˆ‘çš„çœ‹æ³•æ˜¯...`,
            `å“‡ï¼Œ${message} è¿™ä¸ªè¯é¢˜æˆ‘ä»¬å¾—å¥½å¥½èŠèŠï¼`,
            `å˜¿ï¼Œ${message} è¿™ä¸ªé—®é¢˜é—®å¾—å¥½ï¼è®©æˆ‘æƒ³æƒ³...`
        ]
        return responses[Math.floor(Math.random() * responses.length)]
    }

    generateExpertResponse(message, context) {
        const responses = [
            `ä»ä¸“ä¸šè§’åº¦æ¥çœ‹ï¼Œ${message} æ¶‰åŠä»¥ä¸‹å‡ ä¸ªå…³é”®å› ç´ ...`,
            `æ ¹æ®è¡Œä¸šæ ‡å‡†ï¼Œ${message} çš„æœ€ä½³å®è·µæ˜¯...`,
            `åœ¨ä¸“ä¸šé¢†åŸŸå†…ï¼Œ${message} é€šå¸¸éµå¾ªè¿™æ ·çš„åŸåˆ™...`,
            `ä½œä¸ºä¸“å®¶ï¼Œæˆ‘è®¤ä¸º${message} çš„æ ¸å¿ƒé—®é¢˜åœ¨äº...`
        ]
        return responses[Math.floor(Math.random() * responses.length)]
    }

    generateDefaultResponse(message, context) {
        const responses = [
            `æˆ‘æ”¶åˆ°äº†æ‚¨çš„æ¶ˆæ¯ï¼š${message}ã€‚è®©æˆ‘æ€è€ƒä¸€ä¸‹å¦‚ä½•å›å¤...`,
            `å…³äº"${message}"ï¼Œæˆ‘çš„æƒ³æ³•æ˜¯...`,
            `æ„Ÿè°¢æ‚¨çš„æ¶ˆæ¯ã€‚${message} è¿™ä¸ªè¯é¢˜å€¼å¾—æ¢è®¨...`,
            `æˆ‘ç†è§£æ‚¨è¯´çš„æ˜¯${message}ã€‚ä»æˆ‘çš„è§’åº¦æ¥çœ‹...`
        ]
        return responses[Math.floor(Math.random() * responses.length)]
    }

    // æµå¼å“åº”ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
    async *sendMessageStream(agent, message, conversationHistory = []) {
        const fullResponse = await this.sendMessage(agent, message, conversationHistory)

        // æ¨¡æ‹Ÿæµå¼è¾“å‡º
        const words = fullResponse.split(' ')
        for (let i = 0; i < words.length; i++) {
            await new Promise(resolve => setTimeout(resolve, 50 + Math.random() * 100))
            yield words.slice(0, i + 1).join(' ')
        }
    }

    // é€å­—è¾“å‡º - ä¼˜åŒ–ç‰ˆæœ¬
    async outputWordByWord(response, onProgress, settings, thinkingTime) {
        let currentText = ''
        const chars = response.split('')
        let lastUpdateTime = 0
        const UPDATE_INTERVAL = 50 // æœ€å°æ›´æ–°é—´éš”(ms)
        let animationFrameId = null

        for (let i = 0; i < chars.length; i++) {
            currentText += chars[i]

            // ä½¿ç”¨èŠ‚æµæ§åˆ¶æ›´æ–°é¢‘ç‡
            const now = Date.now()
            if (now - lastUpdateTime >= UPDATE_INTERVAL) {
                // ä½¿ç”¨ requestAnimationFrame è¿›è¡Œæ‰¹é‡æ›´æ–°
                if (animationFrameId) {
                    cancelAnimationFrame(animationFrameId)
                }

                animationFrameId = requestAnimationFrame(() => {
                    if (onProgress) {
                        const progressText = this.addResponseMetadata(currentText, settings, thinkingTime, true)
                        onProgress(progressText)
                    }
                })

                lastUpdateTime = now
            }

            // æ¨¡æ‹Ÿæ‰“å­—é€Ÿåº¦
            await new Promise(resolve => setTimeout(resolve, 30 + Math.random() * 30))
        }

        // ç¡®ä¿æœ€ç»ˆæ–‡æœ¬å®Œæ•´æ˜¾ç¤º
        if (currentText !== response) {
            requestAnimationFrame(() => {
                if (onProgress) {
                    const progressText = this.addResponseMetadata(response, settings, thinkingTime, true)
                    onProgress(progressText)
                }
            })
        }

        return this.addResponseMetadata(response, settings, thinkingTime)
    }

    // æ·»åŠ å“åº”å…ƒæ•°æ®
    addResponseMetadata(response, settings, thinkingTime, isPartial = false) {
        // å¤„ç†å¯¹è±¡å“åº”ï¼ˆåŒ…å« response å’Œ tokensï¼‰
        let responseText = response
        let tokens = null

        if (typeof response === 'object' && response !== null) {
            responseText = response.response || ''
            tokens = response.tokens || null
        }

        // å¦‚æœæ²¡æœ‰ä»¤ç‰Œæ•°ï¼Œè¿›è¡Œä¼°ç®—
        if (!tokens) {
            tokens = this.estimateTokens(responseText)
        }

        // åˆ›å»ºå…ƒæ•°æ®å¯¹è±¡
        const metadata = {
            response: responseText,
            tokens: settings.showTokens && !isPartial ? tokens : null,
            thinkingTime: settings.showThinkingTime && !isPartial ? thinkingTime : null
        }

        return metadata
    }

    // æ ¼å¼åŒ–æ€è€ƒæ—¶é—´
    formatThinkingTime(milliseconds) {
        if (milliseconds < 1000) {
            return `${milliseconds}ms`
        } else if (milliseconds < 60000) {
            return `${(milliseconds / 1000).toFixed(1)}s`
        } else {
            const minutes = Math.floor(milliseconds / 60000)
            const seconds = Math.floor((milliseconds % 60000) / 1000)
            return `${minutes}m ${seconds}s`
        }
    }

    // ç”Ÿæˆæ¨èå›å¤ - æ”¯æŒå¹¶å‘è¯·æ±‚
    async generateSuggestedReplies(agent, conversationHistory, settings) {
        const requestId = this.generateRequestId()

        return new Promise((resolve, reject) => {
            const request = {
                id: requestId,
                agent,
                conversationHistory,
                settings,
                resolve,
                reject,
                timestamp: Date.now(),
                type: 'suggestions'
            }

            this.requestQueue.push(request)
            this.processSuggestionQueue()
        })
    }

    // å¤„ç†æ¨èå›å¤è¯·æ±‚é˜Ÿåˆ—
    async processSuggestionQueue() {
        // å¦‚æœå·²è¾¾åˆ°æœ€å¤§å¹¶å‘æ•°æˆ–é˜Ÿåˆ—ä¸ºç©ºï¼Œåˆ™è¿”å›
        if (this.activeRequests.size >= this.maxConcurrentRequests || this.requestQueue.length === 0) {
            return
        }

        // ä»é˜Ÿåˆ—ä¸­å–å‡ºä¸‹ä¸€ä¸ªæ¨èå›å¤è¯·æ±‚
        const suggestionRequest = this.requestQueue.find(req => req.type === 'suggestions')
        if (!suggestionRequest) {
            return
        }

        // ä»é˜Ÿåˆ—ä¸­ç§»é™¤è¯¥è¯·æ±‚
        const queueIndex = this.requestQueue.indexOf(suggestionRequest)
        this.requestQueue.splice(queueIndex, 1)
        this.activeRequests.set(suggestionRequest.id, suggestionRequest)

        try {
            // æ„å»ºæ¨èå›å¤çš„æç¤ºè¯
            const suggestionPrompt = this.buildSuggestionPrompt(suggestionRequest.agent, suggestionRequest.conversationHistory)

            let suggestions
            if (suggestionRequest.settings.apiType === 'network') {
                suggestions = await this.generateNetworkSuggestions(suggestionPrompt, suggestionRequest.settings)
            } else {
                suggestions = await this.generateLocalSuggestions(suggestionPrompt, suggestionRequest.agent)
            }

            suggestionRequest.resolve(suggestions)
        } catch (error) {
            suggestionRequest.reject(error)
        } finally {
            this.activeRequests.delete(suggestionRequest.id)
            // ç»§ç»­å¤„ç†é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯·æ±‚
            this.processSuggestionQueue()
        }
    }

    // æ„å»ºæ¨èå›å¤æç¤ºè¯
    buildSuggestionPrompt(agent, conversationHistory) {
        const lastUserMessage = conversationHistory
            .filter(msg => msg.role === 'assistant')
            .pop()

        const lastMessage = lastUserMessage ? lastUserMessage.content : 'å¼€å§‹å¯¹è¯'
        const agentContext = agent.prompt || ''
        const agentScenario = agent.scenario || ''

        return `è¯·åŸºäºä»¥ä¸‹å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆ4ä¸ªä¸åŒçš„æ¨èå›å¤é€‰é¡¹ã€‚

å¯¹è¯ä¸Šä¸‹æ–‡ï¼š
- æ™ºèƒ½ä½“è§’è‰²ï¼š${agentContext}
- æ™ºèƒ½ä½“æœ€åæ¶ˆæ¯ï¼š"${lastMessage}"

è¦æ±‚ï¼š
1. ç”Ÿæˆ4ä¸ªä¸åŒçš„å›å¤é€‰é¡¹ï¼Œä»ç”¨æˆ·çš„è§†è§’å‡ºå‘
2. æ¯ä¸ªå›å¤åº”è¯¥ç®€æ´æ˜äº†ï¼Œé€‚åˆä½œä¸ºå¿«é€Ÿå›å¤
3. å›å¤åº”è¯¥ç¬¦åˆæ™ºèƒ½ä½“çš„è§’è‰²è®¾å®šï¼Œä½†è¦ä»¥ç”¨æˆ·çš„å£å»è¡¨è¾¾
4. å›å¤åº”è¯¥ä¸å¯¹è¯ä¸Šä¸‹æ–‡ç›¸å…³
5. ä½¿ç”¨ä¸­æ–‡å›å¤
6. ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è¯´æ˜æˆ–æ ¼å¼

è¯·ç›´æ¥è¿”å›4ä¸ªå›å¤é€‰é¡¹ï¼Œæ¯ä¸ªé€‰é¡¹ç”¨æ¢è¡Œåˆ†éš”ï¼Œä¸è¦ä½¿ç”¨æ•°å­—æˆ–é¡¹ç›®ç¬¦å·ã€‚`
    }

    // ç½‘ç»œAPIç”Ÿæˆæ¨èå›å¤
    async generateNetworkSuggestions(suggestionPrompt, settings) {
        const { apiEndpoint, apiKey, modelName } = settings

        if (!apiEndpoint || !apiKey) {
            throw new Error('è¯·é…ç½®APIç«¯ç‚¹å’Œå¯†é’¥')
        }

        const provider = this.detectAPIProvider(apiEndpoint)
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)
        const headers = this.buildRequestHeaders(apiKey, provider)

        const requestBody = {
            model: modelName,
            messages: [
                {
                    role: 'user',
                    content: suggestionPrompt
                }
            ],
            temperature: 0.8, // ç¨é«˜çš„æ¸©åº¦ä»¥è·å¾—å¤šæ ·æ€§
            max_tokens: 200
        }

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            if (!response.ok) {
                throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status}`)
            }

            const data = await response.json()
            const parsed = this.parseResponseContent(data, provider)

            if (!parsed || !parsed.content) {
                throw new Error('æ— æ³•è§£ææ¨èå›å¤')
            }

            return this.parseSuggestedReplies(parsed.content)
        } catch (error) {
            console.error('ç”Ÿæˆæ¨èå›å¤å¤±è´¥:', error)
            throw error
        }
    }

    // æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ¨èå›å¤
    async generateLocalSuggestions(suggestionPrompt, agent) {
        // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await new Promise(resolve => setTimeout(resolve, 500 + Math.random() * 500))

        const context = agent.prompt || ''
        const scenario = agent.scenario || ''

        // åŸºäºæ™ºèƒ½ä½“ç±»å‹ç”Ÿæˆä¸åŒçš„æ¨èå›å¤ - ä»ç”¨æˆ·è§†è§’å‡ºå‘
        let suggestions = []

        if (context.includes('åŠ©æ‰‹') || context.includes('assistant')) {
            suggestions = [
                'è¯·å¸®æˆ‘è¯¦ç»†è§£é‡Šä¸€ä¸‹è¿™ä¸ªé—®é¢˜',
                'æˆ‘æƒ³äº†è§£ä¸€äº›å®ç”¨çš„å»ºè®®',
                'è¿™ä¸ªé—®é¢˜å¯ä»¥ä»å“ªäº›è§’åº¦åˆ†æï¼Ÿ',
                'æˆ‘éœ€è¦æ›´å¤šçš„ä¿¡æ¯æ¥ç†è§£è¿™ä¸ªé—®é¢˜'
            ]
        } else if (context.includes('æœ‹å‹') || context.includes('friend')) {
            suggestions = [
                'å“ˆå“ˆï¼Œè¿™ä¸ªè¯é¢˜çœŸæœ‰æ„æ€ï¼',
                'æœ‹å‹ï¼Œæˆ‘ä¹Ÿæœ‰ç±»ä¼¼çš„æƒ³æ³•',
                'å“‡ï¼Œæˆ‘ä»¬ç»§ç»­èŠè¿™ä¸ªè¯é¢˜å§ï¼',
                'å˜¿ï¼Œè¿™ä¸ªé—®é¢˜æˆ‘ä¹Ÿå¾ˆå¥½å¥‡'
            ]
        } else if (context.includes('ä¸“å®¶') || context.includes('expert')) {
            suggestions = [
                'æˆ‘æƒ³äº†è§£è¿™ä¸ªé—®é¢˜çš„ä¸“ä¸šè§‚ç‚¹',
                'è¯·é—®è¿™ä¸ªé¢†åŸŸçš„æœ€ä½³å®è·µæ˜¯ä»€ä¹ˆï¼Ÿ',
                'ä»ä¸“ä¸šè§’åº¦åº”è¯¥æ€ä¹ˆçœ‹å¾…è¿™ä¸ªé—®é¢˜ï¼Ÿ',
                'æˆ‘æƒ³çŸ¥é“è¿™ä¸ªé—®é¢˜çš„æ ¸å¿ƒè¦ç‚¹'
            ]
        } else {
            suggestions = [
                'æˆ‘æ˜ç™½äº†ï¼Œè®©æˆ‘æƒ³æƒ³',
                'è°¢è°¢åˆ†äº«ï¼Œæˆ‘çš„æƒ³æ³•æ˜¯...',
                'è¿™ä¸ªè¯é¢˜å¾ˆæœ‰æ„æ€ï¼Œç»§ç»­è¯´å§',
                'æˆ‘æ”¶åˆ°äº†ï¼Œè®©æˆ‘å›å¤ä¸€ä¸‹'
            ]
        }

        // æ·»åŠ åœºæ™¯ç›¸å…³çš„å›å¤
        if (scenario) {
            suggestions = suggestions.map(suggestion =>
                `${suggestion} [${scenario}]`
            )
        }

        return suggestions
    }

    // è§£ææ¨èå›å¤
    parseSuggestedReplies(content) {
        // æŒ‰æ¢è¡Œåˆ†å‰²å¹¶æ¸…ç†
        const replies = content
            .split('\n')
            .map(reply => reply.trim())
            .filter(reply => reply.length > 0 && !reply.startsWith('å›å¤') && !reply.match(/^\d+\./))

        // å¦‚æœè§£æå‡ºçš„å›å¤å°‘äº4ä¸ªï¼Œä½¿ç”¨é»˜è®¤å›å¤ - ä»ç”¨æˆ·è§†è§’å‡ºå‘
        if (replies.length < 4) {
            const defaultReplies = [
                'æˆ‘æ˜ç™½äº†',
                'è°¢è°¢åˆ†äº«',
                'è¿™ä¸ªè¯é¢˜å¾ˆæœ‰æ„æ€',
                'è®©æˆ‘æƒ³æƒ³'
            ]
            return defaultReplies.slice(0, 4)
        }

        return replies.slice(0, 4)
    }

    // ç¿»è¯‘æ–‡æœ¬
    async translateText(text, agent, conversationHistory, settings, targetLanguage = 'en') {
        const { apiEndpoint, apiKey, modelName } = settings

        if (!apiEndpoint || !apiKey) {
            throw new Error('è¯·é…ç½®APIç«¯ç‚¹å’Œå¯†é’¥')
        }

        const provider = this.detectAPIProvider(apiEndpoint)
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)
        const headers = this.buildRequestHeaders(apiKey, provider)

        // è¯­è¨€æ˜ å°„
        const languageMap = {
            'en': 'è‹±è¯­',
            'zh': 'ä¸­æ–‡',
            'ja': 'æ—¥è¯­',
            'ko': 'éŸ©è¯­',
            'fr': 'æ³•è¯­',
            'de': 'å¾·è¯­',
            'es': 'è¥¿ç­ç‰™è¯­',
            'ru': 'ä¿„è¯­'
        }

        const targetLanguageName = languageMap[targetLanguage] || 'è‹±è¯­'

        const prompt = `è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ${targetLanguageName}ï¼Œä¿æŒåŸæ–‡çš„è¯­æ°”å’Œé£æ ¼ã€‚

åŸæ–‡ï¼š
${text}

è¦æ±‚ï¼š
1. å‡†ç¡®ç¿»è¯‘åŸæ–‡å«ä¹‰ä¸º${targetLanguageName}
2. ä¿æŒåŸæ–‡çš„è¯­æ°”å’Œé£æ ¼
3. ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è¯´æ˜æˆ–æ ¼å¼
4. ç›´æ¥è¿”å›ç¿»è¯‘ç»“æœ`

        const requestBody = {
            model: modelName,
            messages: [
                {
                    role: 'user',
                    content: prompt
                }
            ],
            temperature: 0.3,
            max_tokens: 1000
        }

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            if (!response.ok) {
                throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status}`)
            }

            const data = await response.json()
            const parsed = this.parseResponseContent(data, provider)
            return parsed.content ? parsed.content.trim() : ''
        } catch (error) {
            console.error('ç¿»è¯‘å¤±è´¥:', error)
            throw error
        }
    }

    // æ‰©å†™æ–‡æœ¬
    async expandText(text, agent, conversationHistory, settings) {
        const { apiEndpoint, apiKey, modelName } = settings

        if (!apiEndpoint || !apiKey) {
            throw new Error('è¯·é…ç½®APIç«¯ç‚¹å’Œå¯†é’¥')
        }

        const provider = this.detectAPIProvider(apiEndpoint)
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)
        const headers = this.buildRequestHeaders(apiKey, provider)

        const prompt = `è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ‰©å†™ï¼Œä½¿å…¶æ›´åŠ è¯¦ç»†å’Œä¸°å¯Œã€‚

åŸæ–‡ï¼š
${text}

è¦æ±‚ï¼š
1. æ‰©å†™å†…å®¹åº”è¯¥ä¸åŸæ–‡ä¸»é¢˜ç›¸å…³
2. å¢åŠ æ›´å¤šçš„ç»†èŠ‚ã€ä¾‹å­å’Œè§£é‡Š
3. ä¿æŒåŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œè¯­æ°”
4. æ‰©å†™åçš„å†…å®¹åº”è¯¥è‡ªç„¶æµç•…
5. ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è¯´æ˜æˆ–æ ¼å¼
6. ç›´æ¥è¿”å›æ‰©å†™ç»“æœ`

        const requestBody = {
            model: modelName,
            messages: [
                {
                    role: 'user',
                    content: prompt
                }
            ],
            temperature: 0.7,
            max_tokens: 2000
        }

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            if (!response.ok) {
                throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status}`)
            }

            const data = await response.json()
            const parsed = this.parseResponseContent(data, provider)
            return parsed.content ? parsed.content.trim() : ''
        } catch (error) {
            console.error('æ‰©å†™å¤±è´¥:', error)
            throw error
        }
    }

    // ä¼˜åŒ–æ–‡æœ¬
    async optimizeText(text, agent, conversationHistory, settings) {
        const { apiEndpoint, apiKey, modelName } = settings

        if (!apiEndpoint || !apiKey) {
            throw new Error('è¯·é…ç½®APIç«¯ç‚¹å’Œå¯†é’¥')
        }

        const provider = this.detectAPIProvider(apiEndpoint)
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)
        const headers = this.buildRequestHeaders(apiKey, provider)

        const prompt = `è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œä¼˜åŒ–ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°ã€å‡†ç¡®å’Œæœ‰è¯´æœåŠ›ã€‚

åŸæ–‡ï¼š
${text}

è¦æ±‚ï¼š
1. æ”¹è¿›è¯­è¨€è¡¨è¾¾ï¼Œä½¿å…¶æ›´åŠ æ¸…æ™°æµç•…
2. ä¿®æ­£è¯­æ³•é”™è¯¯å’Œç”¨è¯ä¸å½“
3. å¢å¼ºé€»è¾‘æ€§å’Œè¯´æœåŠ›
4. ä¿æŒåŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œæ„å›¾
5. ä¼˜åŒ–åçš„å†…å®¹åº”è¯¥ç®€æ´æœ‰åŠ›
6. ä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è¯´æ˜æˆ–æ ¼å¼
7. ç›´æ¥è¿”å›ä¼˜åŒ–ç»“æœ`

        const requestBody = {
            model: modelName,
            messages: [
                {
                    role: 'user',
                    content: prompt
                }
            ],
            temperature: 0.5,
            max_tokens: 1500
        }

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            if (!response.ok) {
                throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status}`)
            }

            const data = await response.json()
            const parsed = this.parseResponseContent(data, provider)
            return parsed.content ? parsed.content.trim() : ''
        } catch (error) {
            console.error('ä¼˜åŒ–å¤±è´¥:', error)
            throw error
        }
    }

    // ç”Ÿæˆé…è‰²æ–¹æ¡ˆ
    async generateColorScheme(prompt, settings) {
        const { apiEndpoint, apiKey, modelName } = settings

        if (!apiEndpoint || !apiKey) {
            throw new Error('è¯·é…ç½®APIç«¯ç‚¹å’Œå¯†é’¥')
        }

        const provider = this.detectAPIProvider(apiEndpoint)
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)
        const headers = this.buildRequestHeaders(apiKey, provider)

        const colorPrompt = `è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ„è±¡"${prompt}"ï¼Œç”Ÿæˆä¸€ç»„å’Œè°çš„é…è‰²æ–¹æ¡ˆã€‚

è¦æ±‚ï¼š
1. è¿”å›2ä¸ªåå…­è¿›åˆ¶é¢œè‰²å€¼ï¼ˆæ ¼å¼å¦‚ #FF5733ï¼‰
2. é¢œè‰²åº”è¯¥å’Œè°æ­é…ï¼Œé€‚åˆä½œä¸ºUIä¸»é¢˜è‰²
3. é¢œè‰²è¦æœ‰è¶³å¤Ÿçš„å¯¹æ¯”åº¦ï¼Œç¡®ä¿å¯è¯»æ€§
4. åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—
5. JSONæ ¼å¼ï¼š{"color1": "#RRGGBB", "color2": "#RRGGBB"}

ç¤ºä¾‹ï¼š
{"color1": "#FF6B6B", "color2": "#4ECDC4"}`

        const requestBody = {
            model: modelName,
            messages: [
                {
                    role: 'user',
                    content: colorPrompt
                }
            ],
            temperature: 0.8,
            max_tokens: 100
        }

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            if (!response.ok) {
                throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status}`)
            }

            const data = await response.json()
            const parsed = this.parseResponseContent(data, provider)

            if (!parsed || !parsed.content) {
                throw new Error('æ— æ³•è§£æé…è‰²æ–¹æ¡ˆ')
            }

            const content = parsed.content

            // å°è¯•è§£æJSON
            try {
                const colorScheme = JSON.parse(content.trim())
                // éªŒè¯é¢œè‰²æ ¼å¼
                if (colorScheme.color1 && colorScheme.color2 && 
                    /^#[0-9A-F]{6}$/i.test(colorScheme.color1) && 
                    /^#[0-9A-F]{6}$/i.test(colorScheme.color2)) {
                    return colorScheme
                } else {
                    throw new Error('é…è‰²æ ¼å¼ä¸æ­£ç¡®')
                }
            } catch (parseError) {
                // å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–é¢œè‰²
                const colorMatches = content.match(/#[0-9A-F]{6}/gi)
                if (colorMatches && colorMatches.length >= 2) {
                    return {
                        color1: colorMatches[0],
                        color2: colorMatches[1]
                    }
                } else {
                    throw new Error('æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆé¢œè‰²')
                }
            }
        } catch (error) {
            console.error('ç”Ÿæˆé…è‰²æ–¹æ¡ˆå¤±è´¥:', error)
            throw error
        }
    }

    // æœ¬åœ°æ¨¡å‹ç”Ÿæˆé…è‰²æ–¹æ¡ˆï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
    async generateLocalColorScheme(prompt) {
        // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await new Promise(resolve => setTimeout(resolve, 800 + Math.random() * 800))

        // åŸºäºå…³é”®è¯ç”Ÿæˆé¢„è®¾é…è‰²
        const colorSchemes = {
            'æµ·æ´‹': { color1: '#006994', color2: '#00D9FF' },
            'æ£®æ—': { color1: '#2D5016', color2: '#73A942' },
            'æ—¥è½': { color1: '#FF6B35', color2: '#F7931E' },
            'å¤œæ™š': { color1: '#2C3E50', color2: '#3498DB' },
            'æ˜¥å¤©': { color1: '#52C234', color2: '#A8E063' },
            'ç§‹å¤©': { color1: '#D84315', color2: '#FF8A65' },
            'ç³–æœ': { color1: '#FF6B9D', color2: '#C44569' },
            'ç§‘æŠ€': { color1: '#00BCD4', color2: '#3F51B5' },
            'æ¸©æš–': { color1: '#FF5722', color2: '#FFC107' },
            'æ¸…å‡‰': { color1: '#00ACC1', color2: '#26C6DA' }
        }

        // æ£€æŸ¥æç¤ºè¯ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        for (const [keyword, colors] of Object.entries(colorSchemes)) {
            if (prompt.includes(keyword)) {
                return colors
            }
        }

        // å¦‚æœæ²¡æœ‰åŒ¹é…çš„å…³é”®è¯ï¼Œè¿”å›éšæœºé…è‰²
        const defaultSchemes = [
            { color1: '#FF6B6B', color2: '#4ECDC4' },
            { color1: '#A8E6CF', color2: '#FFD3B6' },
            { color1: '#FF8B94', color2: '#A8D8EA' },
            { color1: '#C7CEEA', color2: '#FFDAC1' },
            { color1: '#B2E1D4', color2: '#FFAAA5' }
        ]

        return defaultSchemes[Math.floor(Math.random() * defaultSchemes.length)]
    }

    // ç”Ÿæˆé«˜çº§æ¸å˜é…è‰²æ–¹æ¡ˆ
    async generateAdvancedColorScheme(prompt, colorCount, settings) {
        const { apiEndpoint, apiKey, modelName } = settings

        if (!apiEndpoint || !apiKey) {
            throw new Error('è¯·é…ç½®APIç«¯ç‚¹å’Œå¯†é’¥')
        }

        const provider = this.detectAPIProvider(apiEndpoint)
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)
        const headers = this.buildRequestHeaders(apiKey, provider)

        const advancedColorPrompt = `è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ„è±¡"${prompt}"ï¼Œç”Ÿæˆä¸€ç»„${colorCount}ä¸ªé¢œè‰²çš„å’Œè°æ¸å˜é…è‰²æ–¹æ¡ˆã€‚

è¦æ±‚ï¼š
1. è¿”å›${colorCount}ä¸ªåå…­è¿›åˆ¶é¢œè‰²å€¼ï¼ˆæ ¼å¼å¦‚ #FF5733ï¼‰
2. é¢œè‰²åº”è¯¥å’Œè°æ­é…ï¼Œé€‚åˆä½œä¸ºæ¸å˜ä¸»é¢˜è‰²
3. é¢œè‰²ä¹‹é—´è¦æœ‰è‰¯å¥½çš„è¿‡æ¸¡æ•ˆæœ
4. åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—
5. JSONæ ¼å¼ï¼š{"colors": ["#RRGGBB", "#RRGGBB", "#RRGGBB", ...]}

ç¤ºä¾‹ï¼ˆ3è‰²ï¼‰ï¼š
{"colors": ["#FF6B6B", "#4ECDC4", "#45B7D1"]}`

        const requestBody = {
            model: modelName,
            messages: [
                {
                    role: 'user',
                    content: advancedColorPrompt
                }
            ],
            temperature: 0.8,
            max_tokens: 200
        }

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            if (!response.ok) {
                throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status}`)
            }

            const data = await response.json()
            const parsed = this.parseResponseContent(data, provider)

            if (!parsed || !parsed.content) {
                throw new Error('æ— æ³•è§£æé«˜çº§é…è‰²æ–¹æ¡ˆ')
            }

            const content = parsed.content

            // å°è¯•è§£æJSON
            try {
                const colorScheme = JSON.parse(content.trim())
                if (colorScheme.colors && Array.isArray(colorScheme.colors) && colorScheme.colors.length >= colorCount) {
                    // éªŒè¯é¢œè‰²æ ¼å¼
                    const validColors = colorScheme.colors.slice(0, colorCount).filter(color => /^#[0-9A-F]{6}$/i.test(color))
                    if (validColors.length === colorCount) {
                        return validColors
                    }
                }
                throw new Error('é…è‰²æ ¼å¼ä¸æ­£ç¡®')
            } catch (parseError) {
                // å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–é¢œè‰²
                const colorMatches = content.match(/#[0-9A-F]{6}/gi)
                if (colorMatches && colorMatches.length >= colorCount) {
                    return colorMatches.slice(0, colorCount)
                } else {
                    throw new Error('æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆé¢œè‰²')
                }
            }
        } catch (error) {
            console.error('ç”Ÿæˆé«˜çº§é…è‰²æ–¹æ¡ˆå¤±è´¥:', error)
            throw error
        }
    }

    // æœ¬åœ°æ¨¡å‹ç”Ÿæˆé«˜çº§æ¸å˜é…è‰²æ–¹æ¡ˆï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
    async generateLocalAdvancedColorScheme(prompt, colorCount) {
        // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 1000))

        // åŸºäºå…³é”®è¯ç”Ÿæˆé¢„è®¾é…è‰²
        const advancedColorSchemes = {
            'æµ·æ´‹': ['#006994', '#00A8CC', '#00D9FF', '#7FDBFF', '#B3E5FC'],
            'æ£®æ—': ['#2D5016', '#4A7C2E', '#73A942', '#8BC34A', '#A8E063'],
            'æ—¥è½': ['#FF6B35', '#F7931E', '#FFC107', '#FFD54F', '#FFE082'],
            'å¤œæ™š': ['#2C3E50', '#34495E', '#3498DB', '#5DADE2', '#85C1E2'],
            'æ˜¥å¤©': ['#52C234', '#66BB6A', '#4CAF50', '#8BC34A', '#A8E063'],
            'ç§‹å¤©': ['#D84315', '#E64A19', '#FF8A65', '#FFAB91', '#FFCCBC'],
            'ç³–æœ': ['#FF6B9D', '#C44569', '#F8B500', '#FF6B6B', '#C44569'],
            'ç§‘æŠ€': ['#00BCD4', '#3F51B5', '#2196F3', '#03A9F4', '#00ACC1'],
            'æ¸©æš–': ['#FF5722', '#FF7043', '#FF8A65', '#FFAB91', '#FFC107'],
            'æ¸…å‡‰': ['#00ACC1', '#26C6DA', '#4DD0E1', '#80DEEA', '#B2EBF2']
        }

        // æ£€æŸ¥æç¤ºè¯ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        for (const [keyword, colors] of Object.entries(advancedColorSchemes)) {
            if (prompt.includes(keyword)) {
                return colors.slice(0, colorCount)
            }
        }

        // å¦‚æœæ²¡æœ‰åŒ¹é…çš„å…³é”®è¯ï¼Œç”Ÿæˆéšæœºé…è‰²
        const baseColors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F']
        const shuffled = baseColors.sort(() => 0.5 - Math.random())
        return shuffled.slice(0, colorCount)
    }

    // ç”Ÿæˆå…¨å±€ä¸»é¢˜é…è‰²æ–¹æ¡ˆï¼ˆç½‘ç»œAPIï¼‰
    async generateThemeColorScheme(prompt, settings) {
        const { apiEndpoint, apiKey, modelName } = settings

        if (!apiEndpoint || !apiKey) {
            throw new Error('è¯·é…ç½®APIç«¯ç‚¹å’Œå¯†é’¥')
        }

        const provider = this.detectAPIProvider(apiEndpoint)
        const fullUrl = this.buildRequestUrl(apiEndpoint, provider)
        const headers = this.buildRequestHeaders(apiKey, provider)

        const themeColorPrompt = `è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ„è±¡"${prompt}"ï¼Œç”Ÿæˆä¸€ç»„å®Œæ•´çš„UIä¸»é¢˜é…è‰²æ–¹æ¡ˆã€‚

è¦æ±‚ï¼š
1. è¿”å›5ä¸ªåå…­è¿›åˆ¶é¢œè‰²å€¼ï¼ˆæ ¼å¼å¦‚ #FF5733ï¼‰
2. åŒ…å«ï¼šä¸»èƒŒæ™¯è‰²ã€æ¬¡èƒŒæ™¯è‰²ã€ä¸»æ–‡å­—è‰²ã€æ¬¡æ–‡å­—è‰²ã€è¾¹æ¡†è‰²
3. é¢œè‰²åº”è¯¥å’Œè°æ­é…ï¼Œç¡®ä¿è‰¯å¥½çš„å¯è¯»æ€§å’Œå¯¹æ¯”åº¦
4. åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—
5. JSONæ ¼å¼ï¼š{"bgPrimary": "#RRGGBB", "bgSecondary": "#RRGGBB", "textPrimary": "#RRGGBB", "textSecondary": "#RRGGBB", "borderColor": "#RRGGBB"}

ç¤ºä¾‹ï¼š
{"bgPrimary": "#1a1a1a", "bgSecondary": "#2d2d2d", "textPrimary": "#ffffff", "textSecondary": "#b0b0b0", "borderColor": "#404040"}`

        const requestBody = {
            model: modelName,
            messages: [
                {
                    role: 'user',
                    content: themeColorPrompt
                }
            ],
            temperature: 0.8,
            max_tokens: 200
        }

        try {
            const response = await fetch(fullUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody)
            })

            if (!response.ok) {
                throw new Error(`APIè¯·æ±‚å¤±è´¥: ${response.status}`)
            }

            const data = await response.json()
            const parsed = this.parseResponseContent(data, provider)

            if (!parsed || !parsed.content) {
                throw new Error('æ— æ³•è§£æä¸»é¢˜é…è‰²æ–¹æ¡ˆ')
            }

            const content = parsed.content

            // å°è¯•è§£æJSON
            try {
                const themeColors = JSON.parse(content.trim())
                // éªŒè¯é¢œè‰²æ ¼å¼
                const requiredFields = ['bgPrimary', 'bgSecondary', 'textPrimary', 'textSecondary', 'borderColor']
                const isValid = requiredFields.every(field =>
                    themeColors[field] && /^#[0-9A-F]{6}$/i.test(themeColors[field])
                )

                if (isValid) {
                    // è¡¥å……å…¶ä»–å¿…éœ€çš„é¢œè‰²å­—æ®µ
                    return {
                        bgPrimary: themeColors.bgPrimary,
                        bgSecondary: themeColors.bgSecondary,
                        bgTertiary: this.adjustBrightness(themeColors.bgSecondary, 0.1),
                        bgHover: this.adjustBrightness(themeColors.bgSecondary, 0.05),
                        textPrimary: themeColors.textPrimary,
                        textSecondary: themeColors.textSecondary,
                        textTertiary: this.adjustBrightness(themeColors.textSecondary, 0.3),
                        borderColor: themeColors.borderColor,
                        borderLight: this.adjustBrightness(themeColors.borderColor, 0.2),
                        shadow: '0 2px 10px rgba(0, 0, 0, 0.1)',
                        shadowLg: '0 10px 40px rgba(0, 0, 0, 0.15)'
                    }
                } else {
                    throw new Error('ä¸»é¢˜é…è‰²æ ¼å¼ä¸æ­£ç¡®')
                }
            } catch (parseError) {
                // å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–é¢œè‰²
                const colorMatches = content.match(/#[0-9A-F]{6}/gi)
                if (colorMatches && colorMatches.length >= 5) {
                    return {
                        bgPrimary: colorMatches[0],
                        bgSecondary: colorMatches[1],
                        bgTertiary: this.adjustBrightness(colorMatches[1], 0.1),
                        bgHover: this.adjustBrightness(colorMatches[1], 0.05),
                        textPrimary: colorMatches[2],
                        textSecondary: colorMatches[3],
                        textTertiary: this.adjustBrightness(colorMatches[3], 0.3),
                        borderColor: colorMatches[4],
                        borderLight: this.adjustBrightness(colorMatches[4], 0.2),
                        shadow: '0 2px 10px rgba(0, 0, 0, 0.1)',
                        shadowLg: '0 10px 40px rgba(0, 0, 0, 0.15)'
                    }
                } else {
                    throw new Error('æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆé¢œè‰²')
                }
            }
        } catch (error) {
            console.error('ç”Ÿæˆä¸»é¢˜é…è‰²æ–¹æ¡ˆå¤±è´¥:', error)
            throw error
        }
    }

    // æœ¬åœ°æ¨¡å‹ç”Ÿæˆä¸»é¢˜é…è‰²æ–¹æ¡ˆï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰
    async generateLocalThemeColorScheme(prompt) {
        // æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 1000))

        // åŸºäºå…³é”®è¯ç”Ÿæˆé¢„è®¾ä¸»é¢˜é…è‰²
        const themeColorSchemes = {
            'æµ·æ´‹': {
                bgPrimary: '#0f172a',
                bgSecondary: '#1e293b',
                bgTertiary: '#334155',
                bgHover: '#1e3a5f',
                textPrimary: '#f1f5f9',
                textSecondary: '#cbd5e1',
                textTertiary: '#94a3b8',
                borderColor: '#475569',
                borderLight: '#64748b'
            },
            'æ£®æ—': {
                bgPrimary: '#14532d',
                bgSecondary: '#166534',
                bgTertiary: '#15803d',
                bgHover: '#14532d',
                textPrimary: '#f0fdf4',
                textSecondary: '#bbf7d0',
                textTertiary: '#86efac',
                borderColor: '#22c55e',
                borderLight: '#4ade80'
            },
            'æ—¥è½': {
                bgPrimary: '#1c1917',
                bgSecondary: '#292524',
                bgTertiary: '#44403c',
                bgHover: '#78350f',
                textPrimary: '#fef7ed',
                textSecondary: '#fed7aa',
                textTertiary: '#fdba74',
                borderColor: '#ea580c',
                borderLight: '#f97316'
            },
            'å¤œæ™š': {
                bgPrimary: '#0a0a0a',
                bgSecondary: '#1a1a1a',
                bgTertiary: '#2a2a2a',
                bgHover: '#333333',
                textPrimary: '#ffffff',
                textSecondary: '#b0b0b0',
                textTertiary: '#808080',
                borderColor: '#404040',
                borderLight: '#505050'
            },
            'æ˜¥å¤©': {
                bgPrimary: '#f0fdf4',
                bgSecondary: '#dcfce7',
                bgTertiary: '#bbf7d0',
                bgHover: '#86efac',
                textPrimary: '#14532d',
                textSecondary: '#166534',
                textTertiary: '#15803d',
                borderColor: '#22c55e',
                borderLight: '#4ade80'
            },
            'ç§‹å¤©': {
                bgPrimary: '#fef7ed',
                bgSecondary: '#fed7aa',
                bgTertiary: '#fdba74',
                bgHover: '#fb923c',
                textPrimary: '#7c2d12',
                textSecondary: '#9a3412',
                textTertiary: '#c2410c',
                borderColor: '#ea580c',
                borderLight: '#f97316'
            },
            'ç´«ç½—å…°': {
                bgPrimary: '#2e1065',
                bgSecondary: '#4c1d95',
                bgTertiary: '#5b21b6',
                bgHover: '#6d28d9',
                textPrimary: '#faf5ff',
                textSecondary: '#e9d5ff',
                textTertiary: '#d8b4fe',
                borderColor: '#7c3aed',
                borderLight: '#8b5cf6'
            },
            'æ¨±èŠ±': {
                bgPrimary: '#fff1f2',
                bgSecondary: '#ffe4e6',
                bgTertiary: '#fecdd3',
                bgHover: '#fda4af',
                textPrimary: '#881337',
                textSecondary: '#9f1239',
                textTertiary: '#be123c',
                borderColor: '#f43f5e',
                borderLight: '#fb7185'
            },
            'ç§‘æŠ€': {
                bgPrimary: '#0c4a6e',
                bgSecondary: '#075985',
                bgTertiary: '#0369a1',
                bgHover: '#0284c7',
                textPrimary: '#f0f9ff',
                textSecondary: '#bae6fd',
                textTertiary: '#7dd3fc',
                borderColor: '#0ea5e9',
                borderLight: '#38bdf8'
            },
            'æå…‰': {
                bgPrimary: '#0f172a',
                bgSecondary: '#1e293b',
                bgTertiary: '#334155',
                bgHover: '#1e3a5f',
                textPrimary: '#f0f9ff',
                textSecondary: '#bae6fd',
                textTertiary: '#7dd3fc',
                borderColor: '#0ea5e9',
                borderLight: '#38bdf8'
            }
        }

        // æ£€æŸ¥æç¤ºè¯ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        for (const [keyword, colors] of Object.entries(themeColorSchemes)) {
            if (prompt.includes(keyword)) {
                return {
                    ...colors,
                    shadow: '0 2px 10px rgba(0, 0, 0, 0.1)',
                    shadowLg: '0 10px 40px rgba(0, 0, 0, 0.15)'
                }
            }
        }

        // å¦‚æœæ²¡æœ‰åŒ¹é…çš„å…³é”®è¯ï¼Œç”Ÿæˆéšæœºä¸»é¢˜é…è‰²
        const isDark = Math.random() > 0.5
        const baseHue = Math.floor(Math.random() * 360)

        const generateColor = (hue, saturation, lightness) => {
            return this.hslToHex(hue, saturation, lightness)
        }

        return {
            bgPrimary: generateColor(baseHue, 30, isDark ? 10 : 95),
            bgSecondary: generateColor(baseHue, 25, isDark ? 20 : 90),
            bgTertiary: generateColor(baseHue, 20, isDark ? 30 : 85),
            bgHover: generateColor(baseHue, 25, isDark ? 25 : 88),
            textPrimary: generateColor(baseHue, 10, isDark ? 95 : 10),
            textSecondary: generateColor(baseHue, 15, isDark ? 75 : 40),
            textTertiary: generateColor(baseHue, 20, isDark ? 60 : 60),
            borderColor: generateColor(baseHue, 25, isDark ? 40 : 70),
            borderLight: generateColor(baseHue, 20, isDark ? 50 : 65),
            shadow: '0 2px 10px rgba(0, 0, 0, 0.1)',
            shadowLg: '0 10px 40px rgba(0, 0, 0, 0.15)'
        }
    }

    // è¾…åŠ©æ–¹æ³•ï¼šè°ƒæ•´é¢œè‰²äº®åº¦
    adjustBrightness(color, amount) {
        const hex = color.replace('#', '')
        const num = parseInt(hex, 16)
        const r = Math.min(255, Math.max(0, Math.floor((num >> 16) * (1 + amount))))
        const g = Math.min(255, Math.max(0, Math.floor(((num >> 8) & 0x00FF) * (1 + amount))))
        const b = Math.min(255, Math.max(0, Math.floor((num & 0x0000FF) * (1 + amount))))
        return `#${((1 << 24) + (r << 16) + (g << 8) + b).toString(16).slice(1)}`
    }

    // è¾…åŠ©æ–¹æ³•ï¼šHSLè½¬Hex
    hslToHex(h, s, l) {
        s /= 100
        l /= 100
        const a = s * Math.min(l, 1 - l)
        const f = n => {
            const k = (n + h / 30) % 12
            const color = l - a * Math.max(Math.min(k - 3, 9 - k, 1), -1)
            return Math.round(255 * color).toString(16).padStart(2, '0')
        }
        return `#${f(0)}${f(8)}${f(4)}`
    }

    // ä¼°ç®—æ–‡æœ¬çš„ token æ•°ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    estimateTokens(text) {
        if (!text || typeof text !== 'string') {
            return 0
        }

        // æ›´ç²¾ç¡®çš„ token ä¼°ç®—ç®—æ³•
        // åŸºäºå®é™…ç»Ÿè®¡ï¼šä¸­æ–‡å¹³å‡ 1.5-2 tokens/å­—ç¬¦ï¼Œè‹±æ–‡å¹³å‡ 0.75 tokens/å•è¯
        const chineseChars = (text.match(/[\u4e00-\u9fa5]/g) || []).length
        const englishWords = (text.match(/\b[a-zA-Z]+\b/g) || []).length
        const numbers = (text.match(/\b\d+\b/g) || []).length
        const punctuation = (text.match(/[^\w\s]/g) || []).length
        const whitespace = (text.match(/\s/g) || []).length
        const otherChars = text.length - chineseChars - englishWords * 5 - numbers * 3 - punctuation - whitespace

        // è®¡ç®—ä¼°ç®—çš„ token æ•°
        const chineseTokens = chineseChars * 1.8
        const englishTokens = englishWords * 1.3
        const numberTokens = numbers * 1.5
        const punctuationTokens = punctuation * 0.5
        const whitespaceTokens = whitespace * 0.3
        const otherTokens = otherChars * 0.5

        return Math.round(chineseTokens + englishTokens + numberTokens + punctuationTokens + whitespaceTokens + otherTokens)
    }

    // è·å–æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°
    getModelContextWindow(modelName) {
        const model = modelName.toLowerCase()
        
        // GPT-4 ç³»åˆ—
        if (model.includes('gpt-4-turbo') || model.includes('gpt-4-1106')) {
            return 128000
        } else if (model.includes('gpt-4-32k')) {
            return 32768
        } else if (model.includes('gpt-4')) {
            return 8192
        }
        
        // GPT-3.5 ç³»åˆ—
        if (model.includes('gpt-3.5-turbo-16k')) {
            return 16384
        } else if (model.includes('gpt-3.5-turbo')) {
            return 16384
        }
        
        // DeepSeek ç³»åˆ—
        if (model.includes('deepseek')) {
            return 128000
        }
        
        // Claude ç³»åˆ—
        if (model.includes('claude-3-opus')) {
            return 200000
        } else if (model.includes('claude-3-sonnet')) {
            return 200000
        } else if (model.includes('claude-3-haiku')) {
            return 200000
        } else if (model.includes('claude-2')) {
            return 100000
        }
        
        // Gemini ç³»åˆ—
        if (model.includes('gemini-pro')) {
            return 32768
        }
        
        // Qwen ç³»åˆ—ï¼ˆé€šä¹‰åƒé—®ï¼‰
        if (model.includes('qwen')) {
            if (model.includes('128k') || model.includes('long')) {
                return 128000
            } else if (model.includes('32k')) {
                return 32768
            } else {
                return 8192
            }
        }
        
        // GLM ç³»åˆ—ï¼ˆæ™ºè°±ï¼‰
        if (model.includes('glm')) {
            return 128000
        }
        
        // Kimi ç³»åˆ—ï¼ˆæœˆä¹‹æš—é¢ï¼‰
        if (model.includes('kimi')) {
            return 128000
        }
        
        // ç¡…åŸºæµåŠ¨æ¨¡å‹
        if (model.includes('siliconflow') || model.includes('/')) {
            // é»˜è®¤æ”¯æŒ 128K ä¸Šä¸‹æ–‡
            return 128000
        }
        
        // é»˜è®¤å€¼ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
        return 8192
    }

    // åˆå¹¶æ™ºèƒ½ä½“è®¾ç½®å’Œå…¨å±€è®¾ç½®
    mergeAgentSettings(globalSettings, agent) {
        // å¦‚æœæ™ºèƒ½ä½“æ²¡æœ‰å•ç‹¬è®¾ç½®ï¼Œç›´æ¥è¿”å›å…¨å±€è®¾ç½®
        if (!agent || !agent.useCustomApi) {
            return { ...globalSettings }
        }

        // æ™ºèƒ½ä½“æœ‰å•ç‹¬è®¾ç½®ï¼Œåˆå¹¶è®¾ç½®ï¼ˆæ™ºèƒ½ä½“è®¾ç½®ä¼˜å…ˆï¼‰
        const merged = { ...globalSettings }

        // å¦‚æœæ™ºèƒ½ä½“é…ç½®äº†è‡ªå®šä¹‰ APIï¼Œä½¿ç”¨æ™ºèƒ½ä½“çš„é…ç½®
        if (agent.useCustomApi) {
            // ä½¿ç”¨æ™ºèƒ½ä½“çš„è‡ªå®šä¹‰ API é…ç½®
            merged.apiType = 'network'
            
            // æ ¹æ®æœåŠ¡å•†è‡ªåŠ¨è®¾ç½®é»˜è®¤ç«¯ç‚¹
            const endpointMap = {
                openai: 'https://api.openai.com/v1/chat/completions',
                deepseek: 'https://api.deepseek.com/v1/chat/completions',
                anthropic: 'https://api.anthropic.com/v1/messages',
                azure: '',
                google: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent'
            }
            
            // API ç«¯ç‚¹ä¼˜å…ˆä½¿ç”¨æ™ºèƒ½ä½“çš„é…ç½®ï¼Œå¦åˆ™æ ¹æ®æœåŠ¡å•†è‡ªåŠ¨è®¾ç½®
            merged.apiEndpoint = agent.customApiEndpoint || endpointMap[agent.customApiProvider] || globalSettings.apiEndpoint
            
            // API å¯†é’¥ä¼˜å…ˆä½¿ç”¨æ™ºèƒ½ä½“çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€è®¾ç½®
            merged.apiKey = agent.customApiKey || globalSettings.apiKeys?.[agent.customApiProvider] || globalSettings.apiKey
            
            // æ¨¡å‹åç§°ä¼˜å…ˆä½¿ç”¨æ™ºèƒ½ä½“çš„é…ç½®ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€è®¾ç½®
            merged.modelName = agent.customModelName || globalSettings.modelName
        }

        return merged
    }
}